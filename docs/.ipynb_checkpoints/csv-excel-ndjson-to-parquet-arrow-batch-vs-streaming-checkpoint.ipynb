
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# CSV/Excel/NDJSON â†’ Parquet/Arrow: Batch vs Streaming\n",
    "\n",
    "In medical data integration, we often need to convert data from various formats (CSV, Excel, NDJSON) into more efficient columnar formats like Parquet or Arrow. This notebook explores both batch and streaming approaches to handle large medical datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries for data processing and conversion. We'll use pandas for basic operations, pyarrow for Arrow/Parquet handling, and other utilities for file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "First, let's create sample medical datasets in different formats to demonstrate the conversion process. We'll generate synthetic patient data with common medical fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample medical data\n",
    "np.random.seed(42)\n",
    "n_patients = 10000\n",
    "\n",
    "medical_data = {\n",
    "    'patient_id': [f'P{i:06d}' for i in range(n_patients)],\n",
    "    'age': np.random.randint(18, 90, n_patients),\n",
    "    'gender': np.random.choice(['M', 'F'], n_patients),\n",
    "    'diagnosis_code': np.random.choice(['I10', 'E11', 'J44', 'N18', 'F32'], n_patients),\n",
    "    'systolic_bp': np.random.normal(130, 20, n_patients).round(1),\n",
    "    'diastolic_bp': np.random.normal(80, 10, n_patients).round(1),\n",
    "    'lab_value_glucose': np.random.normal(100, 30, n_patients).round(2),\n",
    "    'admission_date': pd.date_range('2020-01-01', periods=n_patients, freq='H')\n",
    "}\n",
    "\n",
    "df_medical = pd.DataFrame(medical_data)\n",
    "print(f\"Created medical dataset with {len(df_medical)} rows and {len(df_medical.columns)} columns\")\n",
    "df_medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Now let's save this data in different source formats that are commonly encountered in medical data integration. We'll create CSV, Excel, and NDJSON files to demonstrate various input scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# Save as CSV\n",
    "df_medical.to_csv('data/medical_data.csv', index=False)\n",
    "print(f\"CSV file size: {os.path.getsize('data/medical_data.csv') / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Save as Excel\n",
    "df_medical.to_excel('data/medical_data.xlsx', index=False)\n",
    "print(f\"Excel file size: {os.path.getsize('data/medical_data.xlsx') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Let's create an NDJSON file where each line represents a patient record. This format is common in streaming medical data systems and log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as NDJSON (Newline Delimited JSON)\n",
    "with open('data/medical_data.ndjson', 'w') as f:\n",
    "    for _, row in df_medical.iterrows():\n",
    "        # Convert datetime to string for JSON serialization\n",
    "        row_dict = row.to_dict()\n",
    "        row_dict['admission_date'] = row_dict['admission_date'].isoformat()\n",
    "        f.write(json.dumps(row_dict) + '\\n')\n",
    "\n",
    "print(f\"NDJSON file size: {os.path.getsize('data/medical_data.ndjson') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Batch Processing Approach\n",
    "\n",
    "Now let's implement the batch processing approach where we load the entire dataset into memory at once. This method is suitable for datasets that fit comfortably in available RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_csv_to_parquet(csv_path, parquet_path):\n",
    "    \"\"\"Convert CSV to Parquet using batch processing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read entire CSV into memory\n",
    "    df = pd.read_csv(csv_path, parse_dates=['admission_date'])\n",
    "    \n",
    "    # Convert to Arrow table and save as Parquet\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Convert CSV to Parquet using batch processing\n",
    "batch_time = batch_csv_to_parquet('data/medical_data.csv', 'data/medical_batch.parquet')\n",
    "print(f\"Batch conversion completed in {batch_time:.2f} seconds\")\n",
    "print(f\"Parquet file size: {os.path.getsize('data/medical_batch.parquet') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Let's also implement batch processing for Excel files, which requires special handling due to the more complex file structure. Excel files often contain multiple sheets and formatting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_excel_to_parquet(excel_path, parquet_path):\n",
    "    \"\"\"Convert Excel to Parquet using batch processing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read entire Excel file into memory\n",
    "    df = pd.read_excel(excel_path)\n",
    "    df['admission_date'] = pd.to_datetime(df['admission_date'])\n",
    "    \n",
    "    # Convert to Arrow table and save as Parquet\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Convert Excel to Parquet using batch processing\n",
    "batch_excel_time = batch_excel_to_parquet('data/medical_data.xlsx', 'data/medical_excel_batch.parquet')\n",
    "print(f\"Excel batch conversion completed in {batch_excel_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Now let's implement batch processing for NDJSON files. Each line needs to be parsed as a separate JSON object and then combined into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_ndjson_to_parquet(ndjson_path, parquet_path):\n",
    "    \"\"\"Convert NDJSON to Parquet using batch processing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read all lines and parse JSON\n",
    "    records = []\n",
    "    with open(ndjson_path, 'r') as f:\n",
    "        for line in f:\n",
    "            records.append(json.loads(line.strip()))\n",
    "    \n",
    "    # Create DataFrame and convert dates\n",
    "    df = pd.DataFrame(records)\n",
    "    df['admission_date'] = pd.to_datetime(df['admission_date'])\n",
    "    \n",
    "    # Convert to Arrow table and save as Parquet\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, parquet_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Convert NDJSON to Parquet using batch processing\n",
    "batch_ndjson_time = batch_ndjson_to_parquet('data/medical_data.ndjson', 'data/medical_ndjson_batch.parquet')\n",
    "print(f\"NDJSON batch conversion completed in {batch_ndjson_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Streaming Processing Approach\n",
    "\n",
    "Now let's implement streaming processing, which processes data in chunks rather than loading everything into memory. This approach is essential for large medical datasets that exceed available RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_csv_to_parquet(csv_path, parquet_path, chunk_size=1000):\n",
    "    \"\"\"Convert CSV to Parquet using streaming processing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Parquet writer\n",
    "    writer = None\n",
    "    \n",
    "    # Process CSV in chunks\n",
    "    for chunk_df in pd.read_csv(csv_path, chunksize=chunk_size, parse_dates=['admission_date']):\n",
    "        # Convert chunk to Arrow table\n",
    "        table = pa.Table.from_pandas(chunk_df)\n",
    "        \n",
    "        # Initialize writer with schema from first chunk\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(parquet_path, table.schema)\n",
    "        \n",
    "        # Write chunk to Parquet file\n",
    "        writer.write_table(table)\n",
    "    \n",
    "    # Close the writer\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Convert CSV to Parquet using streaming processing\n",
    "streaming_time = streaming_csv_to_parquet('data/medical_data.csv', 'data/medical_streaming.parquet')\n",
    "print(f\"Streaming conversion completed in {streaming_time:.2f} seconds\")\n",
    "print(f\"Streaming Parquet file size: {os.path.getsize('data/medical_streaming.parquet') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Let's implement streaming processing for NDJSON files, which is particularly useful for log files and real-time medical data streams. We'll process the file line by line in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_ndjson_to_parquet(ndjson_path, parquet_path, chunk_size=1000):\n",
    "    \"\"\"Convert NDJSON to Parquet using streaming processing\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    writer = None\n",
    "    records_batch = []\n",
    "    \n",
    "    with open(ndjson_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            # Parse JSON record\n",
    "            record = json.loads(line.strip())\n",
    "            records_batch.append(record)\n",
    "            \n",
    "            # Process batch when chunk_size is reached\n",
    "            if len(records_batch) >= chunk_size:\n",
    "                # Create DataFrame from batch\n",
    "                df_batch = pd.DataFrame(records_batch)\n",
    "                df_batch['admission_date'] = pd.to_datetime(df_batch['admission_date'])\n",
    "                \n",
    "                # Convert to Arrow table\n",
    "                table = pa.Table.from_pandas(df_batch)\n",
    "                \n",
    "                # Initialize writer if needed\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(parquet_path, table.schema)\n",
    "                \n",
    "                # Write batch\n",
    "                writer.write_table(table)\n",
    "                records_batch = []  # Reset batch\n",
    "        \n",
    "        # Process remaining records\n",
    "        if records_batch:\n",
    "            df_batch = pd.DataFrame(records_batch)\n",
    "            df_batch['admission_date'] = pd.to_datetime(df_batch['admission_date'])\n",
    "            table = pa.Table.from_pandas(df_batch)\n",
    "            \n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(parquet_path, table.schema)\n",
    "            writer.write_table(table)\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Convert NDJSON to Parquet using streaming processing\n",
    "streaming_ndjson_time = streaming_ndjson_to_parquet('data/medical_data.ndjson', 'data/medical_ndjson_streaming.parquet')\n",
    "print(f\"NDJSON streaming conversion completed in {streaming_ndjson_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Performance Comparison and Memory Usage\n",
    "\n",
    "Let's compare the performance and file sizes of both approaches. We'll also verify that the converted files contain the same data as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison\n",
    "performance_data = {\n",
    "    'Method': ['CSV Batch', 'CSV Streaming', 'NDJSON Batch', 'NDJSON Streaming', 'Excel Batch'],\n",
    "    'Time (seconds)': [batch_time, streaming_time, batch_ndjson_time, streaming_ndjson_time, batch_excel_time]\n",
    "}\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "print(\"Performance Comparison:\")\n",
    "print(performance_df)\n",
    "\n",
    "# File size comparison\n",
    "file_sizes = {\n",
    "    'Original CSV': os.path.getsize('data/medical_data.csv') / 1024 / 1024,\n",
    "    'Original Excel': os.path.getsize('data/medical_data.xlsx') / 1024 / 1024,\n",
    "    'Original NDJSON': os.path.getsize('data/medical_data.ndjson') / 1024 / 1024,\n",
    "    'Batch Parquet': os.path.getsize('data/medical_batch.parquet') / 1024 / 1024,\n",
    "    'Streaming Parquet': os.path.getsize('data/medical_streaming.parquet') / 1024 / 1024\n",
    "}\n",
    "\n",
    "print(\"\\nFile Size Comparison (MB):\")\n",
    "for file_type, size in file_sizes.items():\n",
    "    print(f\"{file_type}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's verify data integrity by reading back one of the Parquet files and comparing it with the original data. This ensures our conversion process preserved all the medical data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data integrity\n",
    "original_df = pd.read_csv('data/medical_data.csv', parse_dates=['admission_date'])\n",
    "parquet_df = pd.read_parquet('data/medical_batch.parquet')\n",
    "\n",
    "print(f\"Original shape: {original_df.shape}\")\n",
    "print(f\"Parquet shape: {parquet_df.shape}\")\n",
    "print(f\"Data types match: {original_df.dtypes.equals(parquet_df.dtypes)}\")\n",
    "print(f\"Data values match: {original_df.equals(parquet_df)}\")\n",
    "\n",
    "# Show compression ratio\n",
    "csv_size = os.path.getsize('data/medical_data.csv')\n",
    "parquet_size = os.path.getsize('data/medical_batch.parquet')\n",
    "compression_ratio = csv_size / parquet_size\n",
    "print(f\"\\nCompression ratio: {compression_ratio:.2f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Working with Arrow Format In-Memory\n",
    "\n",
    "Let's demonstrate working directly with Apache Arrow format in memory, which is beneficial for medical data processing pipelines. Arrow provides zero-copy reads and efficient columnar operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as Arrow table directly\n",
    "arrow_table = pq.read_table('data/medical_batch.parquet')\n",
    "\n",
    "print(f\"Arrow table schema:\")\n",
    "print(arrow_table.schema)\n",
    "print(f\"\\nNumber of rows: {arrow_table.num_rows}\")\n",
    "print(f\"Number of columns: {arrow_table.num_columns}\")\n",
    "\n",
    "# Show memory usage\n",
    "memory_usage = sum(array.nbytes for array in arrow_table.columns)\n",
    "print(f\"Memory usage: {memory_usage / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Let's perform some efficient filtering operations directly on the Arrow table. This demonstrates how columnar format enables fast analytical queries on medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient filtering with Arrow compute functions\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Filter patients with high systolic blood pressure (>150)\n",
    "high_bp_mask = pc.greater(arrow_table['systolic_bp'], 150)\n",
    "high_bp_patients = arrow_table.filter(high_bp_mask)\n",
    "\n",
    "print(f\"Patients with systolic BP > 150: {high_bp_patients.num_rows}\")\n",
    "\n",
    "# Filter by diagnosis code\n",
    "diabetes_mask = pc.equal(arrow_table['diagnosis_code'], 'E11')\n",
    "diabetes_patients = arrow_table.filter(diabetes_mask)\n",
    "\n",
    "print(f\"Patients with diabetes (E11): {diabetes_patients.num_rows}\")\n",
    "\n",
    "# Convert filtered results back to pandas for further analysis\n",
    "diabetes_df = diabetes_patients.to_pandas()\n",
    "print(f\"\\nMean glucose level in diabetes patients: {diabetes_df['lab_value_glucose'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "Let's summarize the key considerations for choosing between batch and streaming approaches in medical data integration. The choice depends on data size, available memory, and processing requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "shutil.rmtree('data', ignore_errors=True)\n",
    "\n",
    "print(\"Best Practices Summary:\")\n",
    "print(\"\\n1. Use BATCH processing when:\")\n",
    "print(\"   - Dataset fits comfortably in memory\")\n",
    "print(\"   - Faster processing is priority\")\n",
    "print(\"   - Simple one-time conversion\")\n",
    "\n",
    "print(\"\\n2. Use STREAMING processing when:\")\n",
    "print(\"   - Dataset is larger than available memory\")\n",
    "print(\"   - Memory efficiency is critical\")\n",
    "print(\"   - Processing real-time data feeds\")\n",
    "print(\"   - Want to minimize memory footprint\")\n",
    "\n",
    "print(\"\\n3. Parquet benefits for medical data:\")\n",
    "print(\"   - Excellent compression (2-5x smaller files)\")\n",
    "print(\"   - Fast analytical queries\")\n",
    "print(\"   - Schema preservation\")\n",
    "print(\"   - Cross-platform compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a medical data processing pipeline that:\n",
    "\n",
    "1. Generate a synthetic dataset of 50,000 patient records with fields: patient_id, age, gender, BMI, blood_pressure_systolic, blood_pressure_diastolic, diagnosis_codes (as a list), and visit_date\n",
    "2. Save this data as both CSV and NDJSON formats\n",
    "3. Implement both batch and streaming conversion functions to convert these files to Parquet format\n",
    "4. Compare the performance, memory usage, and file sizes between the two approaches\n",
    "5. Use Arrow compute functions to filter patients with BMI > 30 and systolic BP > 140 (potential cardiovascular risk)\n",
    "6. Calculate and compare the processing time for this filtering operation on the original CSV vs. the Parquet format\n",
    "\n",
    "Bonus: Implement error handling for corrupted records in the NDJSON file and demonstrate how streaming processing can continue despite individual record failures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
