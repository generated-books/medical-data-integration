
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Patterns: Subsets, Snapshots, Reproducible Filters\n",
    "\n",
    "In medical data integration, it's crucial to create reproducible and well-documented data exports. This notebook covers three essential export patterns: creating data subsets, taking temporal snapshots, and implementing reproducible filters. These techniques ensure data consistency across research teams and enable reliable analysis workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries and creating a sample medical dataset that simulates patient records with temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a synthetic medical dataset with patient information, including demographics, lab results, and visit dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic medical data\n",
    "n_patients = 1000\n",
    "base_date = datetime(2020, 1, 1)\n",
    "\n",
    "# Generate patient data\n",
    "patients_data = {\n",
    "    'patient_id': [f'P_{i:04d}' for i in range(1, n_patients + 1)],\n",
    "    'age': np.random.normal(55, 15, n_patients).astype(int),\n",
    "    'gender': np.random.choice(['M', 'F'], n_patients),\n",
    "    'diagnosis': np.random.choice(['Diabetes', 'Hypertension', 'Asthma', 'Obesity', 'Healthy'], n_patients),\n",
    "    'admission_date': [base_date + timedelta(days=np.random.randint(0, 1095)) for _ in range(n_patients)],\n",
    "    'glucose_level': np.random.normal(100, 20, n_patients),\n",
    "    'blood_pressure_sys': np.random.normal(120, 15, n_patients),\n",
    "    'blood_pressure_dia': np.random.normal(80, 10, n_patients),\n",
    "    'hospital': np.random.choice(['Hospital_A', 'Hospital_B', 'Hospital_C'], n_patients)\n",
    "}\n",
    "\n",
    "df_patients = pd.DataFrame(patients_data)\n",
    "print(f\"Created dataset with {len(df_patients)} patients\")\n",
    "df_patients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the basic characteristics of our dataset to understand its structure and temporal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Date range: {df_patients['admission_date'].min()} to {df_patients['admission_date'].max()}\")\n",
    "print(f\"\\nDiagnosis distribution:\")\n",
    "print(df_patients['diagnosis'].value_counts())\n",
    "print(f\"\\nHospital distribution:\")\n",
    "print(df_patients['hospital'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Data Subsets\n",
    "\n",
    "Data subsets allow us to extract specific portions of the dataset based on clinical criteria. We'll create a function that generates subsets based on multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(df, conditions, subset_name):\n",
    "    \"\"\"\n",
    "    Create a data subset based on multiple conditions\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame\n",
    "    conditions: dict with column names as keys and filter conditions as values\n",
    "    subset_name: string identifier for the subset\n",
    "    \"\"\"\n",
    "    subset_df = df.copy()\n",
    "    filter_log = []\n",
    "    \n",
    "    for column, condition in conditions.items():\n",
    "        initial_count = len(subset_df)\n",
    "        \n",
    "        if isinstance(condition, list):\n",
    "            # Filter for values in list\n",
    "            subset_df = subset_df[subset_df[column].isin(condition)]\n",
    "            filter_log.append(f\"{column} in {condition}: {initial_count} -> {len(subset_df)}\")\n",
    "        elif isinstance(condition, dict):\n",
    "            # Handle range conditions\n",
    "            if 'min' in condition:\n",
    "                subset_df = subset_df[subset_df[column] >= condition['min']]\n",
    "            if 'max' in condition:\n",
    "                subset_df = subset_df[subset_df[column] <= condition['max']]\n",
    "            filter_log.append(f\"{column} range {condition}: {initial_count} -> {len(subset_df)}\")\n",
    "    \n",
    "    print(f\"Subset '{subset_name}' created:\")\n",
    "    for log in filter_log:\n",
    "        print(f\"  {log}\")\n",
    "    print(f\"Final subset size: {len(subset_df)} patients\")\n",
    "    \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a specific subset for diabetic patients with certain age and glucose criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset: Diabetic patients with specific criteria\n",
    "diabetes_conditions = {\n",
    "    'diagnosis': ['Diabetes'],\n",
    "    'age': {'min': 40, 'max': 70},\n",
    "    'glucose_level': {'min': 110}\n",
    "}\n",
    "\n",
    "diabetes_subset = create_subset(df_patients, diabetes_conditions, 'Diabetes_40-70_HighGlucose')\n",
    "diabetes_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create another subset focusing on patients from specific hospitals with hypertension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset: Hypertension patients from specific hospitals\n",
    "hypertension_conditions = {\n",
    "    'diagnosis': ['Hypertension'],\n",
    "    'hospital': ['Hospital_A', 'Hospital_B'],\n",
    "    'blood_pressure_sys': {'min': 130}\n",
    "}\n",
    "\n",
    "hypertension_subset = create_subset(df_patients, hypertension_conditions, 'Hypertension_HospAB_HighBP')\n",
    "hypertension_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Temporal Snapshots\n",
    "\n",
    "Temporal snapshots capture the state of data at specific time points, which is crucial for longitudinal medical studies. We'll create functions to generate snapshots based on admission dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_snapshot(df, snapshot_date, date_column='admission_date'):\n",
    "    \"\"\"\n",
    "    Create a snapshot of data up to a specific date\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame\n",
    "    snapshot_date: datetime object or string\n",
    "    date_column: column name containing dates\n",
    "    \"\"\"\n",
    "    if isinstance(snapshot_date, str):\n",
    "        snapshot_date = pd.to_datetime(snapshot_date)\n",
    "    \n",
    "    snapshot_df = df[df[date_column] <= snapshot_date].copy()\n",
    "    \n",
    "    snapshot_info = {\n",
    "        'snapshot_date': snapshot_date.strftime('%Y-%m-%d'),\n",
    "        'total_records': len(snapshot_df),\n",
    "        'date_range': f\"{snapshot_df[date_column].min().strftime('%Y-%m-%d')} to {snapshot_df[date_column].max().strftime('%Y-%m-%d')}\",\n",
    "        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    return snapshot_df, snapshot_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create snapshots for different time points to see how our patient population evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create snapshots for different time points\n",
    "snapshot_dates = ['2021-01-01', '2021-12-31', '2022-12-31']\n",
    "\n",
    "snapshots = {}\n",
    "for date in snapshot_dates:\n",
    "    snapshot_df, snapshot_info = create_temporal_snapshot(df_patients, date)\n",
    "    snapshots[date] = {'data': snapshot_df, 'info': snapshot_info}\n",
    "    \n",
    "    print(f\"Snapshot {date}:\")\n",
    "    print(f\"  Records: {snapshot_info['total_records']}\")\n",
    "    print(f\"  Date range: {snapshot_info['date_range']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the diagnosis distribution across different snapshots to observe temporal trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare diagnosis distribution across snapshots\n",
    "print(\"Diagnosis distribution across temporal snapshots:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for date, snapshot in snapshots.items():\n",
    "    print(f\"\\nSnapshot {date} ({snapshot['info']['total_records']} patients):\")\n",
    "    diagnosis_counts = snapshot['data']['diagnosis'].value_counts()\n",
    "    for diagnosis, count in diagnosis_counts.items():\n",
    "        percentage = (count / snapshot['info']['total_records']) * 100\n",
    "        print(f\"  {diagnosis}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reproducible Filters\n",
    "\n",
    "Reproducible filters ensure that the same filtering criteria can be applied consistently across different analyses. We'll create a system to save and load filter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReproducibleFilter:\n",
    "    def __init__(self, name, description=\"\"):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.conditions = {}\n",
    "        self.metadata = {\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'version': '1.0'\n",
    "        }\n",
    "    \n",
    "    def add_condition(self, column, condition_type, values):\n",
    "        \"\"\"Add a filter condition\"\"\"\n",
    "        self.conditions[column] = {\n",
    "            'type': condition_type,\n",
    "            'values': values\n",
    "        }\n",
    "        return self\n",
    "    \n",
    "    def apply(self, df):\n",
    "        \"\"\"Apply filter to dataframe\"\"\"\n",
    "        filtered_df = df.copy()\n",
    "        filter_log = []\n",
    "        \n",
    "        for column, condition in self.conditions.items():\n",
    "            initial_count = len(filtered_df)\n",
    "            \n",
    "            if condition['type'] == 'in':\n",
    "                filtered_df = filtered_df[filtered_df[column].isin(condition['values'])]\n",
    "            elif condition['type'] == 'range':\n",
    "                if 'min' in condition['values']:\n",
    "                    filtered_df = filtered_df[filtered_df[column] >= condition['values']['min']]\n",
    "                if 'max' in condition['values']:\n",
    "                    filtered_df = filtered_df[filtered_df[column] <= condition['values']['max']]\n",
    "            elif condition['type'] == 'greater_than':\n",
    "                filtered_df = filtered_df[filtered_df[column] > condition['values']]\n",
    "            elif condition['type'] == 'less_than':\n",
    "                filtered_df = filtered_df[filtered_df[column] < condition['values']]\n",
    "            \n",
    "            filter_log.append(f\"{column}: {initial_count} -> {len(filtered_df)}\")\n",
    "        \n",
    "        return filtered_df, filter_log\n",
    "    \n",
    "    def save_config(self, filepath):\n",
    "        \"\"\"Save filter configuration to JSON file\"\"\"\n",
    "        config = {\n",
    "            'name': self.name,\n",
    "            'description': self.description,\n",
    "            'conditions': self.conditions,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"Filter configuration saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_config(cls, filepath):\n",
    "        \"\"\"Load filter configuration from JSON file\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        filter_obj = cls(config['name'], config['description'])\n",
    "        filter_obj.conditions = config['conditions']\n",
    "        filter_obj.metadata = config['metadata']\n",
    "        \n",
    "        return filter_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create and configure a reproducible filter for high-risk patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reproducible filter for high-risk patients\n",
    "high_risk_filter = ReproducibleFilter(\n",
    "    name=\"High_Risk_Patients_v1\",\n",
    "    description=\"Filter for patients with elevated cardiovascular risk factors\"\n",
    ")\n",
    "\n",
    "# Add conditions to the filter\n",
    "high_risk_filter.add_condition('age', 'greater_than', 50)\n",
    "high_risk_filter.add_condition('blood_pressure_sys', 'greater_than', 130)\n",
    "high_risk_filter.add_condition('diagnosis', 'in', ['Diabetes', 'Hypertension'])\n",
    "\n",
    "print(f\"Created filter: {high_risk_filter.name}\")\n",
    "print(f\"Description: {high_risk_filter.description}\")\n",
    "print(f\"Conditions: {len(high_risk_filter.conditions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the reproducible filter to our dataset and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter\n",
    "filtered_data, log = high_risk_filter.apply(df_patients)\n",
    "\n",
    "print(\"Filter application log:\")\n",
    "for entry in log:\n",
    "    print(f\"  {entry}\")\n",
    "\n",
    "print(f\"\\nFinal filtered dataset: {len(filtered_data)} patients\")\n",
    "print(f\"\\nFiltered data characteristics:\")\n",
    "print(filtered_data[['age', 'diagnosis', 'blood_pressure_sys', 'glucose_level']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll save the filter configuration to ensure reproducibility across different analysis sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filter configuration\n",
    "filter_path = \"high_risk_filter_config.json\"\n",
    "high_risk_filter.save_config(filter_path)\n",
    "\n",
    "# Demonstrate loading the saved configuration\n",
    "loaded_filter = ReproducibleFilter.load_config(filter_path)\n",
    "print(f\"\\nLoaded filter: {loaded_filter.name}\")\n",
    "print(f\"Created at: {loaded_filter.metadata['created_at']}\")\n",
    "print(f\"Conditions loaded: {list(loaded_filter.conditions.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Pipeline Integration\n",
    "\n",
    "Let's combine all three export patterns into a comprehensive pipeline that creates documented, reproducible exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_export_pipeline(df, filter_obj, snapshot_date=None, export_name=\"medical_export\"):\n",
    "    \"\"\"\n",
    "    Comprehensive export pipeline combining filters, snapshots, and documentation\n",
    "    \"\"\"\n",
    "    pipeline_info = {\n",
    "        'export_name': export_name,\n",
    "        'original_records': len(df),\n",
    "        'pipeline_steps': [],\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Step 1: Apply temporal snapshot if specified\n",
    "    current_df = df.copy()\n",
    "    if snapshot_date:\n",
    "        current_df, snapshot_info = create_temporal_snapshot(current_df, snapshot_date)\n",
    "        pipeline_info['pipeline_steps'].append({\n",
    "            'step': 'temporal_snapshot',\n",
    "            'snapshot_date': snapshot_date,\n",
    "            'records_after': len(current_df)\n",
    "        })\n",
    "    \n",
    "    # Step 2: Apply reproducible filter\n",
    "    filtered_df, filter_log = filter_obj.apply(current_df)\n",
    "    pipeline_info['pipeline_steps'].append({\n",
    "        'step': 'reproducible_filter',\n",
    "        'filter_name': filter_obj.name,\n",
    "        'filter_log': filter_log,\n",
    "        'records_after': len(filtered_df)\n",
    "    })\n",
    "    \n",
    "    # Step 3: Generate data hash for integrity checking\n",
    "    data_string = filtered_df.to_string()\n",
    "    data_hash = hashlib.md5(data_string.encode()).hexdigest()\n",
    "    pipeline_info['data_hash'] = data_hash\n",
    "    pipeline_info['final_records'] = len(filtered_df)\n",
    "    \n",
    "    return filtered_df, pipeline_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the complete export pipeline with a temporal snapshot and our high-risk filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute complete export pipeline\n",
    "export_df, export_info = create_export_pipeline(\n",
    "    df_patients, \n",
    "    high_risk_filter, \n",
    "    snapshot_date='2022-01-01',\n",
    "    export_name=\"HighRisk_2022_Export\"\n",
    ")\n",
    "\n",
    "print(f\"Export Pipeline Results:\")\n",
    "print(f\"Export name: {export_info['export_name']}\")\n",
    "print(f\"Original records: {export_info['original_records']}\")\n",
    "print(f\"Final records: {export_info['final_records']}\")\n",
    "print(f\"Data integrity hash: {export_info['data_hash'][:16]}...\")\n",
    "\n",
    "print(f\"\\nPipeline steps:\")\n",
    "for step in export_info['pipeline_steps']:\n",
    "    print(f\"  {step['step']}: {step['records_after']} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save both the exported data and its metadata for complete reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save export data and metadata\n",
    "export_filename = f\"{export_info['export_name']}.csv\"\n",
    "metadata_filename = f\"{export_info['export_name']}_metadata.json\"\n",
    "\n",
    "# Save data\n",
    "export_df.to_csv(export_filename, index=False)\n",
    "\n",
    "# Save metadata\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(export_info, f, indent=2)\n",
    "\n",
    "print(f\"Export completed successfully!\")\n",
    "print(f\"Data saved to: {export_filename}\")\n",
    "print(f\"Metadata saved to: {metadata_filename}\")\n",
    "print(f\"\\nExported dataset preview:\")\n",
    "export_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered three essential export patterns for medical data integration:\n",
    "\n",
    "1. **Data Subsets**: Creating focused datasets based on clinical criteria\n",
    "2. **Temporal Snapshots**: Capturing data state at specific time points for longitudinal studies\n",
    "3. **Reproducible Filters**: Implementing reusable, configurable filtering systems\n",
    "\n",
    "These patterns ensure data consistency, reproducibility, and proper documentation in medical research workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a comprehensive export for a pediatric study using the patterns learned in this notebook:\n",
    "\n",
    "1. **Create a pediatric filter**: Design a ReproducibleFilter for patients aged 0-18 with asthma diagnosis\n",
    "2. **Generate quarterly snapshots**: Create temporal snapshots for each quarter of 2021 (Q1: March 31, Q2: June 30, Q3: September 30, Q4: December 31)\n",
    "3. **Compare trends**: Analyze how the pediatric asthma population changed across these quarters\n",
    "4. **Export pipeline**: Use the export pipeline to create a final dataset combining your filter with the Q4 2021 snapshot\n",
    "5. **Documentation**: Save all configurations and metadata to ensure your analysis can be reproduced\n",
    "\n",
    "**Bonus**: Calculate the percentage change in pediatric asthma cases between Q1 and Q4 2021, and create a filter configuration that could identify patients with concerning vital signs (you define the criteria)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
