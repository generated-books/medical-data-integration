{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Integrity Checks (Hashing) and Small-to-Large File Strategies\n",
    "\n",
    "In medical data integration, ensuring data integrity throughout the pipeline is crucial for patient safety and regulatory compliance. This notebook covers hashing techniques for integrity verification and strategies for handling files of different sizes in medical data workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction to Data Integrity in Medical Context\n",
    "\n",
    "Medical data must maintain its integrity from collection to analysis to ensure accurate diagnoses and treatments. Hash functions provide a mathematical way to verify that data hasn't been corrupted or tampered with during transfer or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries for hashing and file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Basic Hashing Concepts\n",
    "\n",
    "We'll create a simple function to generate MD5 and SHA-256 hashes for different types of data. These algorithms produce fixed-length strings that uniquely represent the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: PAT001\n",
      "MD5 hash: 23a636587eb00292688bacedf0be14db\n",
      "SHA-256 hash: ea711c655944838aa8a4b98030207d6d37e9495e5fba12af2209d8275ec3e41d\n"
     ]
    }
   ],
   "source": [
    "def generate_hash(data: str, algorithm: str = 'sha256') -> str:\n",
    "    \"\"\"Generate hash for given data using specified algorithm.\"\"\"\n",
    "    if algorithm == 'md5':\n",
    "        return hashlib.md5(data.encode()).hexdigest()\n",
    "    elif algorithm == 'sha256':\n",
    "        return hashlib.sha256(data.encode()).hexdigest()\n",
    "    else:\n",
    "        raise ValueError(\"Supported algorithms: 'md5', 'sha256'\")\n",
    "\n",
    "# Test with sample medical data\n",
    "patient_id = \"PAT001\"\n",
    "md5_hash = generate_hash(patient_id, 'md5')\n",
    "sha256_hash = generate_hash(patient_id, 'sha256')\n",
    "\n",
    "print(f\"Original data: {patient_id}\")\n",
    "print(f\"MD5 hash: {md5_hash}\")\n",
    "print(f\"SHA-256 hash: {sha256_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Let's demonstrate how even tiny changes in data result in completely different hashes, which is crucial for detecting data corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Patient: John Doe, Age: 45, Diagnosis: Hypertension\n",
      "Original hash: 7e3942315c13ca93...\n",
      "\n",
      "Modified: Patient: John Doe, Age: 46, Diagnosis: Hypertension\n",
      "Modified hash: e379a88b747881ff...\n",
      "\n",
      "Hashes match: False\n"
     ]
    }
   ],
   "source": [
    "# Original patient record\n",
    "original_record = \"Patient: John Doe, Age: 45, Diagnosis: Hypertension\"\n",
    "original_hash = generate_hash(original_record)\n",
    "\n",
    "# Modified record (small typo)\n",
    "modified_record = \"Patient: John Doe, Age: 46, Diagnosis: Hypertension\"\n",
    "modified_hash = generate_hash(modified_record)\n",
    "\n",
    "print(f\"Original: {original_record}\")\n",
    "print(f\"Original hash: {original_hash[:16]}...\")\n",
    "print(f\"\\nModified: {modified_record}\")\n",
    "print(f\"Modified hash: {modified_hash[:16]}...\")\n",
    "print(f\"\\nHashes match: {original_hash == modified_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## File Hashing for Different File Sizes\n",
    "\n",
    "Now we'll create functions to handle hashing of files efficiently, with different strategies for small and large files. For large files, we'll use chunk-based processing to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small file hash: 39305a36efa785d368b4ef3155b3b70361e8117c9c6ab536591b4293e54b4269\n"
     ]
    }
   ],
   "source": [
    "def hash_small_file(filepath: str, algorithm: str = 'sha256') -> str:\n",
    "    \"\"\"Hash small files by loading entire content into memory.\"\"\"\n",
    "    hasher = hashlib.sha256() if algorithm == 'sha256' else hashlib.md5()\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        content = f.read()\n",
    "        hasher.update(content)\n",
    "    \n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# Create a sample small medical data file\n",
    "sample_data = {\n",
    "    'patient_id': ['PAT001', 'PAT002', 'PAT003'],\n",
    "    'age': [45, 32, 67],\n",
    "    'diagnosis': ['Hypertension', 'Diabetes', 'Arthritis']\n",
    "}\n",
    "df = pd.DataFrame(sample_data)\n",
    "df.to_csv('small_medical_data.csv', index=False)\n",
    "\n",
    "small_file_hash = hash_small_file('small_medical_data.csv')\n",
    "print(f\"Small file hash: {small_file_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For large medical files (like medical imaging data or genomic sequences), we need a chunk-based approach to avoid loading the entire file into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large file hash: 05f1d7f74340f83940810eb3526c0b8837ac7fa010d2b859ef278ce46a74a614\n",
      "File size: 662183 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rober\\AppData\\Local\\Temp\\ipykernel_1000\\2689253228.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'timestamp': pd.date_range('2023-01-01', periods=10000, freq='H')\n"
     ]
    }
   ],
   "source": [
    "def hash_large_file(filepath: str, chunk_size: int = 8192, algorithm: str = 'sha256') -> str:\n",
    "    \"\"\"Hash large files using chunk-based processing.\"\"\"\n",
    "    hasher = hashlib.sha256() if algorithm == 'sha256' else hashlib.md5()\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            hasher.update(chunk)\n",
    "    \n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# Create a larger sample file to demonstrate\n",
    "large_data = pd.DataFrame({\n",
    "    'patient_id': [f'PAT{i:05d}' for i in range(10000)],\n",
    "    'measurement_1': np.random.normal(100, 15, 10000),\n",
    "    'measurement_2': np.random.normal(80, 10, 10000),\n",
    "    'timestamp': pd.date_range('2023-01-01', periods=10000, freq='H')\n",
    "})\n",
    "large_data.to_csv('large_medical_data.csv', index=False)\n",
    "\n",
    "large_file_hash = hash_large_file('large_medical_data.csv')\n",
    "print(f\"Large file hash: {large_file_hash}\")\n",
    "print(f\"File size: {os.path.getsize('large_medical_data.csv')} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Smart File Processing Strategy\n",
    "\n",
    "Let's create an intelligent function that automatically chooses the appropriate hashing strategy based on file size. This is particularly useful in medical data pipelines where file sizes can vary dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small file processing:\n",
      "  filepath: small_medical_data.csv\n",
      "  file_size_bytes: 91\n",
      "  strategy_used: small_file\n",
      "  hash: 39305a36efa785d368b4ef3155b3b70361e8117c9c6ab536591b4293e54b4269\n",
      "  processing_time_seconds: 0.0\n",
      "\n",
      "Large file processing:\n",
      "  filepath: large_medical_data.csv\n",
      "  file_size_bytes: 662183\n",
      "  strategy_used: small_file\n",
      "  hash: 05f1d7f74340f83940810eb3526c0b8837ac7fa010d2b859ef278ce46a74a614\n",
      "  processing_time_seconds: 0.0018\n"
     ]
    }
   ],
   "source": [
    "def smart_file_hash(filepath: str, size_threshold: int = 10*1024*1024) -> Dict:\n",
    "    \"\"\"Choose hashing strategy based on file size (default threshold: 10MB).\"\"\"\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if file_size < size_threshold:\n",
    "        strategy = \"small_file\"\n",
    "        file_hash = hash_small_file(filepath)\n",
    "    else:\n",
    "        strategy = \"large_file_chunked\"\n",
    "        file_hash = hash_large_file(filepath)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'filepath': filepath,\n",
    "        'file_size_bytes': file_size,\n",
    "        'strategy_used': strategy,\n",
    "        'hash': file_hash,\n",
    "        'processing_time_seconds': round(processing_time, 4)\n",
    "    }\n",
    "\n",
    "# Test with both files\n",
    "small_result = smart_file_hash('small_medical_data.csv')\n",
    "large_result = smart_file_hash('large_medical_data.csv')\n",
    "\n",
    "print(\"Small file processing:\")\n",
    "for key, value in small_result.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nLarge file processing:\")\n",
    "for key, value in large_result.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Creating an Integrity Manifest\n",
    "\n",
    "In medical data integration, it's common to create manifest files that store hash values for multiple files. This allows for batch integrity verification across entire datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manifest created with 2 files\n",
      "Creation time: 2025-09-13T17:35:55.212573\n"
     ]
    }
   ],
   "source": [
    "def create_integrity_manifest(file_paths: List[str], manifest_path: str = 'integrity_manifest.json') -> Dict:\n",
    "    \"\"\"Create a manifest file with hash values for multiple files.\"\"\"\n",
    "    manifest = {\n",
    "        'created_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'files': []\n",
    "    }\n",
    "    \n",
    "    for filepath in file_paths:\n",
    "        if os.path.exists(filepath):\n",
    "            file_info = smart_file_hash(filepath)\n",
    "            manifest['files'].append(file_info)\n",
    "    \n",
    "    # Save manifest to file\n",
    "    with open(manifest_path, 'w') as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "# Create manifest for our sample files\n",
    "files_to_check = ['small_medical_data.csv', 'large_medical_data.csv']\n",
    "manifest = create_integrity_manifest(files_to_check)\n",
    "\n",
    "print(f\"Manifest created with {len(manifest['files'])} files\")\n",
    "print(f\"Creation time: {manifest['created_timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Verifying File Integrity\n",
    "\n",
    "Now we'll create a function to verify file integrity by comparing current hash values with those stored in our manifest. This is essential for detecting data corruption in medical data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification completed at: 2025-09-13T17:35:55.234712\n",
      "Files passed: 2/2\n",
      "Files failed: 0\n",
      "Files missing: 0\n",
      "\n",
      "Detailed results:\n",
      "  small_medical_data.csv: PASSED\n",
      "  large_medical_data.csv: PASSED\n"
     ]
    }
   ],
   "source": [
    "def verify_integrity(manifest_path: str = 'integrity_manifest.json') -> Dict:\n",
    "    \"\"\"Verify file integrity against stored manifest.\"\"\"\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        manifest = json.load(f)\n",
    "    \n",
    "    verification_results = {\n",
    "        'verification_timestamp': pd.Timestamp.now().isoformat(),\n",
    "        'total_files': len(manifest['files']),\n",
    "        'passed': 0,\n",
    "        'failed': 0,\n",
    "        'missing': 0,\n",
    "        'details': []\n",
    "    }\n",
    "    \n",
    "    for file_info in manifest['files']:\n",
    "        filepath = file_info['filepath']\n",
    "        expected_hash = file_info['hash']\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            verification_results['missing'] += 1\n",
    "            status = 'MISSING'\n",
    "            current_hash = None\n",
    "        else:\n",
    "            current_result = smart_file_hash(filepath)\n",
    "            current_hash = current_result['hash']\n",
    "            \n",
    "            if current_hash == expected_hash:\n",
    "                verification_results['passed'] += 1\n",
    "                status = 'PASSED'\n",
    "            else:\n",
    "                verification_results['failed'] += 1\n",
    "                status = 'FAILED'\n",
    "        \n",
    "        verification_results['details'].append({\n",
    "            'filepath': filepath,\n",
    "            'status': status,\n",
    "            'expected_hash': expected_hash[:16] + '...',\n",
    "            'current_hash': current_hash[:16] + '...' if current_hash else None\n",
    "        })\n",
    "    \n",
    "    return verification_results\n",
    "\n",
    "# Verify integrity of our files\n",
    "verification = verify_integrity()\n",
    "\n",
    "print(f\"Verification completed at: {verification['verification_timestamp']}\")\n",
    "print(f\"Files passed: {verification['passed']}/{verification['total_files']}\")\n",
    "print(f\"Files failed: {verification['failed']}\")\n",
    "print(f\"Files missing: {verification['missing']}\")\n",
    "\n",
    "print(\"\\nDetailed results:\")\n",
    "for detail in verification['details']:\n",
    "    print(f\"  {detail['filepath']}: {detail['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Let's simulate a file corruption scenario to see how our integrity verification detects changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File corrupted (age changed from 45 to 999)\n",
      "\n",
      "Re-running integrity verification...\n",
      "Files passed: 1/2\n",
      "Files failed: 1\n",
      "\n",
      "Corruption detected in: small_medical_data.csv\n",
      "Expected hash: 39305a36efa785d3...\n",
      "Current hash:  af90fa5a280c0b40...\n"
     ]
    }
   ],
   "source": [
    "# Simulate file corruption by modifying the small file\n",
    "original_df = pd.read_csv('small_medical_data.csv')\n",
    "corrupted_df = original_df.copy()\n",
    "corrupted_df.loc[0, 'age'] = 999  # Introduce corruption\n",
    "corrupted_df.to_csv('small_medical_data.csv', index=False)\n",
    "\n",
    "print(\"File corrupted (age changed from 45 to 999)\")\n",
    "print(\"\\nRe-running integrity verification...\")\n",
    "\n",
    "verification_after_corruption = verify_integrity()\n",
    "\n",
    "print(f\"Files passed: {verification_after_corruption['passed']}/{verification_after_corruption['total_files']}\")\n",
    "print(f\"Files failed: {verification_after_corruption['failed']}\")\n",
    "\n",
    "for detail in verification_after_corruption['details']:\n",
    "    if detail['status'] == 'FAILED':\n",
    "        print(f\"\\nCorruption detected in: {detail['filepath']}\")\n",
    "        print(f\"Expected hash: {detail['expected_hash']}\")\n",
    "        print(f\"Current hash:  {detail['current_hash']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance of different hashing strategies to understand when to use each approach. This helps optimize medical data processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance comparison results:\n",
      "          strategy chunk_size  processing_time\n",
      "  load_entire_file        N/A           0.0010\n",
      "chunked_processing       1024           0.0000\n",
      "chunked_processing       4096           0.0021\n",
      "chunked_processing       8192           0.0000\n",
      "chunked_processing      16384           0.0000\n"
     ]
    }
   ],
   "source": [
    "def performance_comparison(filepath: str, chunk_sizes: List[int] = [1024, 4096, 8192, 16384]) -> pd.DataFrame:\n",
    "    \"\"\"Compare performance of different chunk sizes for large file hashing.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test small file strategy\n",
    "    start_time = time.time()\n",
    "    hash_small_file(filepath)\n",
    "    small_file_time = time.time() - start_time\n",
    "    \n",
    "    results.append({\n",
    "        'strategy': 'load_entire_file',\n",
    "        'chunk_size': 'N/A',\n",
    "        'processing_time': small_file_time\n",
    "    })\n",
    "    \n",
    "    # Test different chunk sizes\n",
    "    for chunk_size in chunk_sizes:\n",
    "        start_time = time.time()\n",
    "        hash_large_file(filepath, chunk_size)\n",
    "        chunk_time = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'strategy': 'chunked_processing',\n",
    "            'chunk_size': chunk_size,\n",
    "            'processing_time': chunk_time\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compare performance on our large file\n",
    "perf_results = performance_comparison('large_medical_data.csv')\n",
    "perf_results['processing_time'] = perf_results['processing_time'].round(4)\n",
    "\n",
    "print(\"Performance comparison results:\")\n",
    "print(perf_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Finally, let's clean up the temporary files we created during this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: small_medical_data.csv\n",
      "Removed: large_medical_data.csv\n",
      "Removed: integrity_manifest.json\n",
      "\n",
      "Cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "temp_files = ['small_medical_data.csv', 'large_medical_data.csv', 'integrity_manifest.json']\n",
    "\n",
    "for filepath in temp_files:\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "        print(f\"Removed: {filepath}\")\n",
    "\n",
    "print(\"\\nCleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Hash functions provide reliable integrity verification** for medical data files\n",
    "2. **Different strategies are needed** for small vs. large files to optimize performance\n",
    "3. **Manifest files enable batch integrity verification** across multiple files\n",
    "4. **Chunk-based processing** prevents memory issues with large medical datasets\n",
    "5. **Regular integrity checks** are essential in medical data pipelines for patient safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a medical data integrity monitoring system with the following requirements:\n",
    "\n",
    "1. **Create sample medical files**: Generate 3 CSV files with different sizes:\n",
    "   - Small file: 100 patient records with basic demographics\n",
    "   - Medium file: 5,000 patient records with vital signs data\n",
    "   - Large file: 50,000 patient records with lab results\n",
    "\n",
    "2. **Implement automated integrity checking**: Create a function that:\n",
    "   - Automatically detects the appropriate hashing strategy for each file\n",
    "   - Creates a timestamped integrity manifest\n",
    "   - Logs all operations with file sizes and processing times\n",
    "\n",
    "3. **Simulate real-world scenarios**: \n",
    "   - Verify integrity of all files initially (should pass)\n",
    "   - Simulate data corruption in one file by modifying a few values\n",
    "   - Run integrity verification again and identify which file was corrupted\n",
    "   - Generate a summary report showing which files passed/failed verification\n",
    "\n",
    "4. **Bonus challenge**: Implement a feature that can restore corrupted files from backup copies and re-verify integrity.\n",
    "\n",
    "This exercise will help you understand how integrity checking works in practice and how to build robust medical data processing pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
