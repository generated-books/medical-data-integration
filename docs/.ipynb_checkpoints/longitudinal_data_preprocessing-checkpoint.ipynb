
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Normalization, Time Alignment, Resampling, and Windowing for Longitudinal Data\n",
    "\n",
    "This notebook covers essential preprocessing techniques for longitudinal medical data, including unit normalization, time alignment, resampling, and windowing. These techniques are crucial for preparing time-series medical data for analysis and integration across different sources and measurement systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary libraries for working with time-series data and create some synthetic longitudinal medical data to demonstrate the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create synthetic longitudinal medical data representing different patients with measurements taken at irregular intervals. This simulates real-world scenarios where data collection frequency varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic longitudinal data for 3 patients\n",
    "def create_patient_data(patient_id, start_date, n_days, measurement_frequency):\n",
    "    dates = []\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date <= start_date + timedelta(days=n_days):\n",
    "        if np.random.random() < measurement_frequency:\n",
    "            dates.append(current_date)\n",
    "        current_date += timedelta(hours=6)  # Check every 6 hours\n",
    "    \n",
    "    n_measurements = len(dates)\n",
    "    \n",
    "    # Simulate different vital signs with different units and scales\n",
    "    data = {\n",
    "        'patient_id': [patient_id] * n_measurements,\n",
    "        'timestamp': dates,\n",
    "        'heart_rate_bpm': np.random.normal(70 + patient_id * 10, 10, n_measurements),\n",
    "        'blood_pressure_systolic_mmHg': np.random.normal(120 + patient_id * 5, 15, n_measurements),\n",
    "        'temperature_celsius': np.random.normal(36.5 + patient_id * 0.2, 0.5, n_measurements),\n",
    "        'weight_kg': np.random.normal(70 + patient_id * 5, 2, n_measurements)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate data for 3 patients\n",
    "start_date = datetime(2023, 1, 1)\n",
    "patients_data = []\n",
    "\n",
    "for patient_id in range(1, 4):\n",
    "    patient_data = create_patient_data(patient_id, start_date, 30, 0.3)\n",
    "    patients_data.append(patient_data)\n",
    "\n",
    "# Combine all patient data\n",
    "raw_data = pd.concat(patients_data, ignore_index=True)\n",
    "print(f\"Generated {len(raw_data)} measurements for {raw_data['patient_id'].nunique()} patients\")\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the raw data to understand the irregular timing and different scales of measurements across patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw longitudinal data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "measurements = ['heart_rate_bpm', 'blood_pressure_systolic_mmHg', 'temperature_celsius', 'weight_kg']\n",
    "\n",
    "for i, measurement in enumerate(measurements):\n",
    "    for patient_id in raw_data['patient_id'].unique():\n",
    "        patient_data = raw_data[raw_data['patient_id'] == patient_id]\n",
    "        axes[i].scatter(patient_data['timestamp'], patient_data[measurement], \n",
    "                       label=f'Patient {patient_id}', alpha=0.7)\n",
    "    \n",
    "    axes[i].set_title(measurement.replace('_', ' ').title())\n",
    "    axes[i].legend()\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unit Normalization\n",
    "\n",
    "Unit normalization is essential when combining measurements with different scales and units. We'll demonstrate both min-max normalization and z-score standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for normalization (excluding non-numeric columns)\n",
    "numeric_columns = ['heart_rate_bpm', 'blood_pressure_systolic_mmHg', 'temperature_celsius', 'weight_kg']\n",
    "normalization_data = raw_data.copy()\n",
    "\n",
    "print(\"Original data statistics:\")\n",
    "print(normalization_data[numeric_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply min-max normalization to scale all values between 0 and 1. This is useful when we want to preserve the relative relationships while ensuring all features have the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Min-Max normalization\n",
    "scaler_minmax = MinMaxScaler()\n",
    "normalized_minmax = normalization_data.copy()\n",
    "normalized_minmax[numeric_columns] = scaler_minmax.fit_transform(normalization_data[numeric_columns])\n",
    "\n",
    "print(\"Min-Max normalized data statistics:\")\n",
    "print(normalized_minmax[numeric_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply z-score standardization, which transforms data to have zero mean and unit variance. This is particularly useful when dealing with normally distributed medical measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Z-score standardization\n",
    "scaler_standard = StandardScaler()\n",
    "normalized_zscore = normalization_data.copy()\n",
    "normalized_zscore[numeric_columns] = scaler_standard.fit_transform(normalization_data[numeric_columns])\n",
    "\n",
    "print(\"Z-score standardized data statistics:\")\n",
    "print(normalized_zscore[numeric_columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the effect of different normalization techniques on our data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare normalization techniques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original data\n",
    "for col in numeric_columns:\n",
    "    axes[0].hist(normalization_data[col], alpha=0.5, label=col, bins=20)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Min-Max normalized\n",
    "for col in numeric_columns:\n",
    "    axes[1].hist(normalized_minmax[col], alpha=0.5, label=col, bins=20)\n",
    "axes[1].set_title('Min-Max Normalized')\n",
    "axes[1].legend()\n",
    "\n",
    "# Z-score standardized\n",
    "for col in numeric_columns:\n",
    "    axes[2].hist(normalized_zscore[col], alpha=0.5, label=col, bins=20)\n",
    "axes[2].set_title('Z-score Standardized')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Alignment\n",
    "\n",
    "Time alignment involves synchronizing measurements from different sources or patients to a common time reference. We'll demonstrate aligning data to the earliest timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time alignment - align all patients to a common time reference\n",
    "def align_timestamps(data, reference_time=None):\n",
    "    aligned_data = data.copy()\n",
    "    \n",
    "    if reference_time is None:\n",
    "        reference_time = data['timestamp'].min()\n",
    "    \n",
    "    # Calculate time difference in hours from reference time\n",
    "    aligned_data['time_from_start_hours'] = (aligned_data['timestamp'] - reference_time).dt.total_seconds() / 3600\n",
    "    \n",
    "    return aligned_data, reference_time\n",
    "\n",
    "aligned_data, ref_time = align_timestamps(raw_data)\n",
    "\n",
    "print(f\"Reference time: {ref_time}\")\n",
    "print(f\"Time range: {aligned_data['time_from_start_hours'].min():.1f} to {aligned_data['time_from_start_hours'].max():.1f} hours\")\n",
    "aligned_data[['patient_id', 'timestamp', 'time_from_start_hours', 'heart_rate_bpm']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the time alignment affects our data representation, showing measurements as a function of time from the common reference point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time-aligned data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for patient_id in aligned_data['patient_id'].unique():\n",
    "    patient_data = aligned_data[aligned_data['patient_id'] == patient_id]\n",
    "    plt.scatter(patient_data['time_from_start_hours'], patient_data['heart_rate_bpm'], \n",
    "               label=f'Patient {patient_id}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Time from Start (hours)')\n",
    "plt.ylabel('Heart Rate (BPM)')\n",
    "plt.title('Time-Aligned Heart Rate Measurements')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resampling\n",
    "\n",
    "Resampling involves converting irregular time series data to regular intervals. We'll demonstrate both upsampling (interpolation) and downsampling (aggregation) techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling - convert to regular intervals\n",
    "def resample_patient_data(patient_data, freq='6H', method='linear'):\n",
    "    \"\"\"\n",
    "    Resample patient data to regular intervals\n",
    "    freq: pandas frequency string (e.g., '6H' for 6 hours)\n",
    "    method: interpolation method ('linear', 'nearest', 'cubic')\n",
    "    \"\"\"\n",
    "    # Set timestamp as index\n",
    "    patient_data_indexed = patient_data.set_index('timestamp')\n",
    "    \n",
    "    # Resample to regular intervals and interpolate\n",
    "    resampled = patient_data_indexed.resample(freq).mean()\n",
    "    \n",
    "    # Interpolate missing values\n",
    "    for col in numeric_columns:\n",
    "        if col in resampled.columns:\n",
    "            resampled[col] = resampled[col].interpolate(method=method)\n",
    "    \n",
    "    # Reset index and add patient_id back\n",
    "    resampled = resampled.reset_index()\n",
    "    resampled['patient_id'] = patient_data['patient_id'].iloc[0]\n",
    "    \n",
    "    return resampled\n",
    "\n",
    "# Resample data for each patient\n",
    "resampled_data_list = []\n",
    "for patient_id in aligned_data['patient_id'].unique():\n",
    "    patient_data = aligned_data[aligned_data['patient_id'] == patient_id]\n",
    "    resampled_patient = resample_patient_data(patient_data, freq='6H')\n",
    "    resampled_data_list.append(resampled_patient)\n",
    "\n",
    "resampled_data = pd.concat(resampled_data_list, ignore_index=True)\n",
    "\n",
    "print(f\"Original data points: {len(aligned_data)}\")\n",
    "print(f\"Resampled data points: {len(resampled_data)}\")\n",
    "resampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the original irregular data with the resampled regular interval data to see the effect of interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs resampled data\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original irregular data\n",
    "for patient_id in aligned_data['patient_id'].unique():\n",
    "    patient_data = aligned_data[aligned_data['patient_id'] == patient_id]\n",
    "    axes[0].scatter(patient_data['timestamp'], patient_data['heart_rate_bpm'], \n",
    "                   label=f'Patient {patient_id}', alpha=0.7)\n",
    "\n",
    "axes[0].set_title('Original Irregular Data')\n",
    "axes[0].legend()\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Resampled regular data\n",
    "for patient_id in resampled_data['patient_id'].unique():\n",
    "    patient_data = resampled_data[resampled_data['patient_id'] == patient_id]\n",
    "    axes[1].plot(patient_data['timestamp'], patient_data['heart_rate_bpm'], \n",
    "                'o-', label=f'Patient {patient_id}', alpha=0.7)\n",
    "\n",
    "axes[1].set_title('Resampled Regular Intervals (6H)')\n",
    "axes[1].legend()\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Windowing\n",
    "\n",
    "Windowing involves creating overlapping or non-overlapping time windows for analysis. This is crucial for feature extraction and temporal pattern analysis in longitudinal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing - create time windows for analysis\n",
    "def create_time_windows(data, window_size_hours=24, overlap_hours=12):\n",
    "    \"\"\"\n",
    "    Create overlapping time windows from longitudinal data\n",
    "    window_size_hours: size of each window in hours\n",
    "    overlap_hours: overlap between consecutive windows in hours\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    \n",
    "    for patient_id in data['patient_id'].unique():\n",
    "        patient_data = data[data['patient_id'] == patient_id].sort_values('timestamp')\n",
    "        \n",
    "        if len(patient_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        start_time = patient_data['timestamp'].min()\n",
    "        end_time = patient_data['timestamp'].max()\n",
    "        \n",
    "        current_window_start = start_time\n",
    "        window_id = 0\n",
    "        \n",
    "        while current_window_start + timedelta(hours=window_size_hours) <= end_time:\n",
    "            window_end = current_window_start + timedelta(hours=window_size_hours)\n",
    "            \n",
    "            # Extract data within the window\n",
    "            window_data = patient_data[\n",
    "                (patient_data['timestamp'] >= current_window_start) & \n",
    "                (patient_data['timestamp'] < window_end)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(window_data) > 0:\n",
    "                # Calculate window statistics\n",
    "                window_stats = {\n",
    "                    'patient_id': patient_id,\n",
    "                    'window_id': window_id,\n",
    "                    'window_start': current_window_start,\n",
    "                    'window_end': window_end,\n",
    "                    'n_measurements': len(window_data)\n",
    "                }\n",
    "                \n",
    "                # Add statistics for each measurement\n",
    "                for col in numeric_columns:\n",
    "                    if col in window_data.columns:\n",
    "                        window_stats[f'{col}_mean'] = window_data[col].mean()\n",
    "                        window_stats[f'{col}_std'] = window_data[col].std()\n",
    "                        window_stats[f'{col}_min'] = window_data[col].min()\n",
    "                        window_stats[f'{col}_max'] = window_data[col].max()\n",
    "                \n",
    "                windows.append(window_stats)\n",
    "            \n",
    "            # Move to next window\n",
    "            current_window_start += timedelta(hours=window_size_hours - overlap_hours)\n",
    "            window_id += 1\n",
    "    \n",
    "    return pd.DataFrame(windows)\n",
    "\n",
    "# Create 24-hour windows with 12-hour overlap\n",
    "windowed_data = create_time_windows(resampled_data, window_size_hours=24, overlap_hours=12)\n",
    "\n",
    "print(f\"Created {len(windowed_data)} windows from {len(resampled_data)} data points\")\n",
    "windowed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the windowed data to show how statistical features are extracted from each time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize windowed statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "statistics = ['mean', 'std', 'min', 'max']\n",
    "measurement = 'heart_rate_bpm'\n",
    "\n",
    "for i, stat in enumerate(statistics):\n",
    "    col_name = f'{measurement}_{stat}'\n",
    "    \n",
    "    for patient_id in windowed_data['patient_id'].unique():\n",
    "        patient_windows = windowed_data[windowed_data['patient_id'] == patient_id]\n",
    "        axes[i].plot(patient_windows['window_id'], patient_windows[col_name], \n",
    "                    'o-', label=f'Patient {patient_id}', alpha=0.7)\n",
    "    \n",
    "    axes[i].set_title(f'Heart Rate {stat.upper()} per Window')\n",
    "    axes[i].set_xlabel('Window ID')\n",
    "    axes[i].set_ylabel(f'{stat.upper()} (BPM)')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate how to combine all preprocessing steps into a complete pipeline for processing longitudinal medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete preprocessing pipeline\n",
    "def preprocess_longitudinal_data(raw_data, normalization_method='zscore', \n",
    "                                resample_freq='6H', window_size_hours=24, \n",
    "                                overlap_hours=12):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for longitudinal medical data\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Time alignment...\")\n",
    "    aligned_data, ref_time = align_timestamps(raw_data)\n",
    "    \n",
    "    print(\"Step 2: Unit normalization...\")\n",
    "    if normalization_method == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    normalized_data = aligned_data.copy()\n",
    "    normalized_data[numeric_columns] = scaler.fit_transform(aligned_data[numeric_columns])\n",
    "    \n",
    "    print(\"Step 3: Resampling...\")\n",
    "    resampled_list = []\n",
    "    for patient_id in normalized_data['patient_id'].unique():\n",
    "        patient_data = normalized_data[normalized_data['patient_id'] == patient_id]\n",
    "        resampled_patient = resample_patient_data(patient_data, freq=resample_freq)\n",
    "        resampled_list.append(resampled_patient)\n",
    "    \n",
    "    resampled_data = pd.concat(resampled_list, ignore_index=True)\n",
    "    \n",
    "    print(\"Step 4: Windowing...\")\n",
    "    windowed_data = create_time_windows(resampled_data, window_size_hours, overlap_hours)\n",
    "    \n",
    "    return {\n",
    "        'aligned': aligned_data,\n",
    "        'normalized': normalized_data,\n",
    "        'resampled': resampled_data,\n",
    "        'windowed': windowed_data,\n",
    "        'scaler': scaler,\n",
    "        'reference_time': ref_time\n",
    "    }\n",
    "\n",
    "# Apply complete pipeline\n",
    "processed_data = preprocess_longitudinal_data(raw_data)\n",
    "\n",
    "print(f\"\\nPipeline completed:\")\n",
    "print(f\"- Raw data: {len(raw_data)} measurements\")\n",
    "print(f\"- Resampled data: {len(processed_data['resampled'])} measurements\")\n",
    "print(f\"- Windowed features: {len(processed_data['windowed'])} windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's examine the final processed features that could be used for downstream analysis or machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the final processed features\n",
    "final_features = processed_data['windowed']\n",
    "\n",
    "# Select feature columns (exclude metadata)\n",
    "feature_columns = [col for col in final_features.columns if \n",
    "                  col.endswith(('_mean', '_std', '_min', '_max'))]\n",
    "\n",
    "print(f\"Available features: {len(feature_columns)}\")\n",
    "print(\"Feature columns:\")\n",
    "for col in feature_columns[:8]:  # Show first 8 features\n",
    "    print(f\"  - {col}\")\n",
    "print(f\"  ... and {len(feature_columns) - 8} more\")\n",
    "\n",
    "# Show feature matrix\n",
    "print(\"\\nFeature matrix shape:\", final_features[feature_columns].shape)\n",
    "final_features[['patient_id', 'window_id'] + feature_columns[:4]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered four essential preprocessing techniques for longitudinal medical data:\n",
    "\n",
    "1. **Unit Normalization**: We demonstrated min-max scaling and z-score standardization to handle measurements with different units and scales\n",
    "2. **Time Alignment**: We aligned timestamps to a common reference point to enable comparison across patients\n",
    "3. **Resampling**: We converted irregular time series to regular intervals using interpolation\n",
    "4. **Windowing**: We created overlapping time windows and extracted statistical features for analysis\n",
    "\n",
    "These techniques are fundamental for preparing longitudinal medical data for integration, analysis, and machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Using the techniques learned in this notebook, complete the following tasks:\n",
    "\n",
    "1. **Generate new synthetic data**: Create longitudinal data for 5 patients over 60 days with measurements including heart rate, blood pressure, temperature, and glucose levels (mg/dL with values around 100Â±20).\n",
    "\n",
    "2. **Apply different normalization techniques**: Compare the results of min-max normalization, z-score standardization, and robust scaling (using `RobustScaler` from sklearn) on your data.\n",
    "\n",
    "3. **Experiment with resampling**: Try different resampling frequencies (2H, 12H, 24H) and interpolation methods ('linear', 'cubic', 'nearest'). Visualize the differences.\n",
    "\n",
    "4. **Custom windowing**: Create a windowing function that extracts additional features such as slope (trend), area under the curve, and peak detection for each window.\n",
    "\n",
    "5. **Pipeline evaluation**: Create a function that evaluates the quality of your preprocessing pipeline by calculating metrics such as data coverage (percentage of time with data), interpolation error, and feature correlation before and after processing.\n",
    "\n",
    "Submit your solution showing the preprocessed data characteristics, visualizations comparing different approaches, and your evaluation metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
