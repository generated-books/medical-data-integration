{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87404224-b84b-409f-8683-c4a243d29722",
   "metadata": {},
   "source": [
    "# Generating a Jupyter book about Python basics\n",
    "In this notebook we will generate a Jupyter book using a large language model. We use Claude 4 Opus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "752e974d-9aaf-44aa-80fb-01a042cf5774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.106.1', '0.60.0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anthropic\n",
    "import openai\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from IPython.display import Markdown, display\n",
    "openai.__version__, anthropic.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d3a85-c367-41f8-94dd-87e0b4187740",
   "metadata": {},
   "source": [
    "## Defining the content of the book\n",
    "The topic of the book will be specified and also the table of contents and some extra hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2375a5-d453-4c58-9b31-9baf19914e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Foundations of Medical Data Integration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2c99e3-ae03-41d0-b6dd-350181cdd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table of contents must be a markdown list with * at the beginning of every line.\n",
    "toc = \"\"\"\n",
    "* Files, Folders, and Filesystems\n",
    "* Path handling and portable I/O (pathlib)\n",
    "* Sidecar metadata (JSON/YAML), data dictionaries\n",
    "* Local and cloud storage with fsspec (s3fs/gcsfs/adlfs), streaming I/O\n",
    "* Next-cloud access to files\n",
    "* Integrity checks (hashing), small-to-large file strategies\n",
    "* Reading/writing CSV/NDJSON (pandas/polars), dtype control, missing values\n",
    "* Schemas and validation (pandera), coercion, constraints\n",
    "* Efficient storage: Parquet/Arrow, compression, partitioning\n",
    "* Joins/merges, grouping, windowing; SQL-in-notebooks with DuckDB\n",
    "* Export patterns: subsets, snapshots, reproducible filters\n",
    "* numpy/xarray fundamentals: shapes, dtypes, chunking\n",
    "* Array storage: HDF5/Zarr; compression (blosc/zstd), chunk size trade-offs\n",
    "* Image I/O: tifffile/imageio; DICOM with pydicom; OME-TIFF/OME-Zarr basics\n",
    "* CSV/Excel/NDJSON → Parquet/Arrow (batch vs streaming)\n",
    "* In-notebook SQL analytics and joins with DuckDB/SQLite\n",
    "* DICOM → OME-TIFF/OME-Zarr; NIfTI round-trips for neuro\n",
    "* De-identification workflows for EHR, DICOM, and text (hashing, redaction, re-keying)\n",
    "* NLP pipelines for clinical notes (spaCy, rule-based extraction, terminology normalization)\n",
    "* Timezones, units, and categorical normalization\n",
    "* Unit normalization, time alignment, resampling, and windowing for longitudinal data\n",
    "* Entity resolution and patient matching (splink, recordlinkage) with evaluation\n",
    "* Out-of-core/parallel conversion with Dask; progress, retries, idempotency\n",
    "* Map to clinical vocabularies (SNOMED/LOINC/ICD/RxNorm) using Python lookup tables/services\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb318ff-20d4-45be-b309-624faaa357d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_hints = \"\"\"\n",
    "By the end of every notebook, add a markdown cell with the headline \"## Exercise\" \n",
    "and an exercise description allowing the reader to practically apply what they just\n",
    "learned in the notebook.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07f0b9-9eda-47c2-9e1b-0873930ec5b0",
   "metadata": {},
   "source": [
    "We will also specify the location where to store the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea1686b-9098-429a-b216-121a3f46f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"\"\n",
    "repository_url = \"https://github.com/generated-books/medical-data-integration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef14a82-c1ac-4367-a83f-067923eae779",
   "metadata": {},
   "source": [
    "We will use this language model to generate the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e3b22c-f57c-41c1-8a13-8049e23b3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"claude-sonnet-4-20250514\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903cd45-0b6b-4090-8597-cc4f644cae38",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Here we create some helper functions for prompting and for file format handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e686a0f1-9561-426a-b5b8-d222f7966109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_chatGPT(message:str, model=\"gpt-4o-2024-05-13\"):\n",
    "    \"\"\"\n",
    "    A prompt helper function that sends a message to openAI\n",
    "    and returns only the text response.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import openai\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    client = openai.OpenAI()\n",
    "    \n",
    "    # submit prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=message\n",
    "    )\n",
    "    \n",
    "    # extract answer\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea088a7-f187-4c4e-a269-833e14a957d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_claude(message:str, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"\n",
    "    A prompt helper function that sends a message to anthropic\n",
    "    and returns only the text response.\n",
    "\n",
    "    Example models: claude-3-5-sonnet-20240620 or claude-3-opus-20240229\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from anthropic import Anthropic\n",
    "    \n",
    "    # convert message in the right format if necessary\n",
    "    if isinstance(message, str):\n",
    "        message = [{\"role\": \"user\", \"content\": message}]\n",
    "        \n",
    "    # setup connection to the LLM\n",
    "    client = Anthropic()\n",
    "    \n",
    "    results = []\n",
    "    with client.messages.stream(\n",
    "        max_tokens=16000,\n",
    "        messages=message,\n",
    "        model=model,\n",
    "    ) as stream:\n",
    "      for text in stream.text_stream:\n",
    "          results.append(text)\n",
    "    return \"\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721c3126-67a5-4a65-8bfb-c054bd4dd153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"gpt\" in model:\n",
    "    prompt = partial(prompt_gpt, model=model)\n",
    "else:\n",
    "    prompt = partial(prompt_claude, model=model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbf5ff4b-15f7-4b68-8c53-9f5e095776d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How are you doing today? Is there anything I can help you with?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a137f79a-287c-4283-8e66-53c8863cdc86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_with_memory(message:str):\n",
    "    \"\"\"\n",
    "    This function allows to use an LLMs in a chat-mode. \n",
    "    The LLM is equipped with some memory, \n",
    "    so that we can refer back for former conversation steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert message in the right format and store it in memory\n",
    "    question = {\"role\": \"user\", \"content\": message}\n",
    "    chat_history.append(question)\n",
    "    \n",
    "    # receive answer\n",
    "    response = prompt(chat_history)\n",
    "    \n",
    "    # convert answer in the right format and store it in memory\n",
    "    answer = {\"role\": \"assistant\", \"content\": response}\n",
    "    chat_history.append(answer)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8c8b27a-77f0-431d-a68c-ee4b19f647b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_json(test_string):\n",
    "    \"\"\"This function returns if a string is formatted json.\"\"\"\n",
    "    import json\n",
    "    try:\n",
    "        json.loads(test_string)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def ensure_json(notebook):\n",
    "    \"\"\"This function makes sure that the passed notebook is indeed a json-formatted ipynb file.\"\"\"\n",
    "    if is_valid_json(notebook):\n",
    "        return notebook\n",
    "        \n",
    "    return prompt(f\"\"\"\n",
    "Take the following text and extract the Jupyter \n",
    "notebook ipynb/json from it:\n",
    "\n",
    "{notebook}\n",
    "\n",
    "Make sure the output is in ipynb/json format. \n",
    "Respond only the JSON content.\n",
    "\"\"\").strip(\"```json\").strip(\"```python\").strip(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a5b4c-29d7-4ec7-b7e2-f232a695f960",
   "metadata": {},
   "source": [
    "## Context\n",
    "Here we provide some context to the language model. As gpt4 and claude have different APIs for providing system messages, we instead use this message to start the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeab283c-8453-4cd7-a0d8-0583c2bd64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"\n",
    "You are data scientist and statistician, and you work in the medical domain. \n",
    "You have didactic skills and you can explain data analysis very well.\n",
    "You are about to write a Jupyter book consisting of multiple Jupyter notebooks about a given topic.\n",
    "\n",
    "In front of every code-cell, add a markdown cell with an explanation of the next code cell. Write 1-3 sentences in these markdown cells.\n",
    "When writing a notebook, always keep the code in the code cells concise. \n",
    "Do only one thing and let the user see the intermediate result.\n",
    "Then, continue with the next thing in a new code cell.\n",
    "\n",
    "{extra_hints}\n",
    "\n",
    "Confirm this with \"ok\".\n",
    "\"\"\"\n",
    "\n",
    "chat_history = [{\"role\": \"user\", \"content\": system_message}, {\"role\": \"assistant\", \"content\": \"ok\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c0439-09db-4eb7-b13c-be4d5eb15137",
   "metadata": {},
   "source": [
    "We just test if the chat mode works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37c6e8e1-04eb-43b4-b259-b47f6a0907e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Robert! Nice to meet you. I\\'m ready to help you create a comprehensive Jupyter book with multiple notebooks on your chosen topic. \\n\\nAs confirmed, I\\'ll structure each notebook with:\\n- Clear markdown explanations (1-3 sentences) before each code cell\\n- Concise, focused code cells that demonstrate one concept at a time\\n- Intermediate results displayed for better understanding\\n- An \"## Exercise\" section at the end of each notebook for hands-on practice\\n\\nWhat medical data analysis topic would you like to focus on for this Jupyter book?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_memory(\"Hi, my name is Robert Haase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3153f9b3-1de7-4e9d-9e63-ded31362680a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Robert Haase.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_memory(\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80975a9-6422-4158-847a-a77519b8194d",
   "metadata": {},
   "source": [
    "## Chatting about book content\n",
    "We start chatting with the LLM about the book's content. It is key that the LLM _knows_ about all the content of the book before it starts generating the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edefdb37-602d-4864-8ab3-f590bc6c8ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on your comprehensive curriculum for \"Foundations of Medical Data Integration,\" here are the key Python libraries that would be relevant:\n",
       "\n",
       "## Core Data Handling & I/O\n",
       "- **pathlib** - Modern path handling\n",
       "- **pandas/polars** - Tabular data manipulation\n",
       "- **numpy/xarray** - Array operations and n-dimensional data\n",
       "- **fsspec, s3fs, gcsfs, adlfs** - Unified filesystem interfaces\n",
       "- **dask** - Parallel and out-of-core computing\n",
       "\n",
       "## File Formats & Storage\n",
       "- **pyarrow** - Arrow format and Parquet I/O\n",
       "- **h5py/tables** - HDF5 storage\n",
       "- **zarr** - Chunked array storage\n",
       "- **tifffile/imageio** - Image I/O\n",
       "- **pydicom** - DICOM medical imaging\n",
       "- **nibabel** - NIfTI neuroimaging format\n",
       "- **ome-zarr** - OME-ZARR for microscopy\n",
       "\n",
       "## Data Validation & Schemas\n",
       "- **pandera** - Data validation and schemas\n",
       "- **pydantic** - Data validation using Python type annotations\n",
       "- **jsonschema** - JSON schema validation\n",
       "\n",
       "## Database & Query\n",
       "- **duckdb** - In-process analytical database\n",
       "- **sqlite3** - Lightweight database\n",
       "- **sqlalchemy** - SQL toolkit\n",
       "\n",
       "## Medical/Clinical Specific\n",
       "- **spacy** - NLP processing\n",
       "- **splink/recordlinkage** - Entity resolution and record matching\n",
       "- **cryptography/hashlib** - Hashing and de-identification\n",
       "\n",
       "## Metadata & Configuration\n",
       "- **pyyaml** - YAML parsing\n",
       "- **json** - JSON handling\n",
       "- **toml** - Configuration files\n",
       "\n",
       "## Utilities & Quality\n",
       "- **tqdm** - Progress bars\n",
       "- **pytest** - Testing framework\n",
       "- **logging** - Logging and monitoring\n",
       "- **blosc/zstd** - Compression algorithms\n",
       "\n",
       "This creates a solid foundation for teaching medical data integration workflows from basic file handling through advanced clinical data processing pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(prompt_with_memory(f\"\"\"\n",
    "I would like to teach others in {topic} and cover these aspects:\n",
    "{toc}\n",
    "\n",
    "Therefore, it would be great to have training material in the form of a Jupyter book.\n",
    "\n",
    "Which Python libraries are relevant in this context? Do not write any Python code yet.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eccf60-0cd3-4600-939a-7b94e00624db",
   "metadata": {},
   "source": [
    "## Generating the book\n",
    "Here we start generating the notebooks for the content listed in the table of contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be6fefdf-c1cc-4ce0-9521-7f8f2b6b3feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrity checks (hashing), small-to-large file strategies : docs\\integrity_checks_hashing_file_strategies.ipynb\n",
      "Reading/writing CSV/NDJSON (pandas/polars), dtype control, missing values : docs\\data_io_csv_ndjson_pandas_polars_dtypes_missing.ipynb\n",
      "Schemas and validation (pandera), coercion, constraints : docs\\pandera_schemas_validation_coercion_constraints.ipynb\n",
      "Efficient storage: Parquet/Arrow, compression, partitioning : docs\\efficient_storage_parquet_arrow_compression_partitioning.ipynb\n",
      "Joins/merges, grouping, windowing; SQL-in-notebooks with DuckDB : docs\\joins-grouping-windowing-duckdb-sql.ipynb\n",
      "Export patterns: subsets, snapshots, reproducible filters : docs\\export_patterns_subsets_snapshots_reproducible_filters.ipynb\n",
      "numpy/xarray fundamentals: shapes, dtypes, chunking : docs\\numpy-xarray-fundamentals-shapes-dtypes-chunking.ipynb\n",
      "Array storage: HDF5/Zarr; compression (blosc/zstd), chunk size trade-offs : docs\\Array-Storage-HDF5-Zarr-Compression-Chunking-Analysis.ipynb\n",
      "Image I/O: tifffile/imageio; DICOM with pydicom; OME-TIFF/OME-Zarr basics : docs\\image_io_tifffile_imageio_dicom_pydicom_ome_basics.ipynb\n",
      "CSV/Excel/NDJSON → Parquet/Arrow (batch vs streaming) : docs\\csv-excel-ndjson-to-parquet-arrow-batch-vs-streaming.ipynb\n",
      "In-notebook SQL analytics and joins with DuckDB/SQLite : docs\\sql-analytics-joins-duckdb-sqlite.ipynb\n",
      "DICOM → OME-TIFF/OME-Zarr; NIfTI round-trips for neuro : docs\\DICOM-to-OME-TIFF-OME-Zarr-NIfTI-neuro-roundtrips.ipynb\n",
      "De-identification workflows for EHR, DICOM, and text (hashing, redaction, re-keying) : docs\\De-identification_Workflows_EHR_DICOM_Text.ipynb\n",
      "NLP pipelines for clinical notes (spaCy, rule-based extraction, terminology normalization) : docs\\clinical_nlp_pipelines_spacy_extraction_normalization.ipynb\n",
      "Timezones, units, and categorical normalization : docs\\data_preprocessing_timezones_units_categorical.ipynb\n",
      "Unit normalization, time alignment, resampling, and windowing for longitudinal data : docs\\longitudinal_data_preprocessing.ipynb\n",
      "Entity resolution and patient matching (splink, recordlinkage) with evaluation : docs\\entity_resolution_patient_matching_splink_recordlinkage_evaluation.ipynb\n",
      "Out-of-core/parallel conversion with Dask; progress, retries, idempotency : docs\\dask-out-of-core-parallel-conversion-progress-retries-idempotency.ipynb\n",
      "Map to clinical vocabularies (SNOMED/LOINC/ICD/RxNorm) using Python lookup tables/services : docs\\clinical_vocabulary_mapping_python.ipynb\n"
     ]
    }
   ],
   "source": [
    "contents = toc.strip(\"\\n\").strip(\"* \").split(\"\\n* \")\n",
    "\n",
    "for i, subtopic in enumerate(contents):\n",
    "    if i < 5:\n",
    "        continue\n",
    "    notebook = ensure_json(prompt(\n",
    "        [{\"role\": \"user\", \"content\": system_message},\n",
    "         {\"role\": \"assistant\", \"content\": \"ok\"},\n",
    "         {\"role\": \"user\", \"content\": f\"\"\"\n",
    "    Please write a Jupyter notebook in json format about \"{subtopic}\" as part of a course about {topic}.\n",
    "    Respond only the JSON content.\n",
    "    \"\"\"}])).strip(\"```json\").strip(\"```python\").strip(\"```\")\n",
    "\n",
    "    # f\"{i:02}_\" + \n",
    "    filename = Path(base_dir) / \"docs\" / prompt(f\"What would be a good filename for '{subtopic}' jupyter notebook? Make sure it contains no spaces and ends with .ipynb . Respond with the filename only.\")\n",
    "\n",
    "    directory = directory = Path(filename).parent\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(notebook)\n",
    "\n",
    "    print(subtopic, \":\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e183b86-e01e-4baa-8a1e-388f77875c13",
   "metadata": {},
   "source": [
    "## Generating additional text and config files\n",
    "We would like to build the book automatically, and we also need some introduction texts and documentation. Now that the individual notebooks have been built, we can generate those additional files as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f69c914-e7e4-431d-82dc-a8a6c9c90b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs\\intro.md\n",
      "docs\\_toc.yml\n",
      "docs\\requirements.txt\n",
      "docs\\_config.yml\n",
      ".github\\workflows\\book.yml\n",
      "readme.md\n"
     ]
    }
   ],
   "source": [
    "docs_folder = Path(base_dir) / \"docs\"\n",
    "today = datetime.date.today().strftime(\"%B %d, %Y\")\n",
    "\n",
    "more_files = {\n",
    "    Path(base_dir) / \"docs\" / \"intro.md\": \n",
    "f\"\"\"\n",
    "Create a intro.md file for a jupyter book that contains all Jupyter notebooks we just created. \n",
    "The introduction should give an overview in text form and with bullet points linking to the notebooks.\n",
    "Mention that the entire book is AI-generated.\n",
    "The repository url of the book is `{repository_url}`.\n",
    "Mention that the `generator.ipynb` file in the github repository contains all the code used for generating the book. Add a link to this file.\n",
    "Respond the content of this file only.\n",
    "\"\"\",\n",
    "    \n",
    "    Path(base_dir) / \"docs\" / \"_toc.yml\": \n",
    "\"\"\"\n",
    "Build a table of contents in Jupytyer book yml format.\n",
    "First, mention the intro.md file.\n",
    "Please give me the list of all notebook filenames we just created. \n",
    "Put them in a _yml file for a Jupyter book.\n",
    "Respond the content of this file only.\n",
    "\"\"\",\n",
    "\n",
    "    Path(base_dir) / \"docs\" / \"requirements.txt\":\n",
    "f\"\"\"\n",
    "A requirements.txt file in the `docs` folder containing all python libraries used in this Jupyter book.\n",
    "Respond the content of this file only.\n",
    "\"\"\",\n",
    "    \n",
    "    Path(base_dir) / \"docs\" / \"_config.yml\": \n",
    "f\"\"\"\n",
    "Create a minimal config.yml file for the jupyter book.\n",
    "The book will be uploaded to this github repository: {repository_url}\n",
    "Make sure the notebooks will be executed when the book is built.\n",
    "The icon for the book is saved in ../icon.png\n",
    "Note that today is {today}.\n",
    "Respond the content of this file only.\n",
    "\"\"\",\n",
    "    \n",
    "    Path(base_dir) / \".github\" / \"workflows\" / \"book.yml\": \n",
    "f\"\"\"\n",
    "Write a Github workflow file that builds the book and uploads the content to the gh_pages branch.\n",
    "The book is stored in the `{docs_folder}` folder of the respository.\n",
    "Respond the content of this file only.\n",
    "\"\"\",\n",
    "\n",
    "    Path(base_dir) / \"readme.md\": \n",
    "f\"\"\"\n",
    "Create a readme.md file for the jupyter book. \n",
    "Give instructions how to build the book.\n",
    "Mention that the entire book is AI-generated. \n",
    "Mention that the `generator.ipynb` file in the github repository contains all the code used for generating the book.\n",
    "Respond the content of this file only.\n",
    "\"\"\",\n",
    "\n",
    "}\n",
    "\n",
    "for filename, task in more_files.items():\n",
    "    file_content = prompt_with_memory(task)\n",
    "\n",
    "    directory = Path(filename).parent\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(file_content)\n",
    "\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce81eb-5734-4d54-a7f9-801327a8e0b7",
   "metadata": {},
   "source": [
    "## Chat history\n",
    "For documentation purposes, we output the entire chat with the LLM. Note: The notebooks were generated without storing the notebooks in the chat-history because that would make the history too quickly too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecab490c-0cf0-4bf2-8266-585c2450d7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': '\\nYou are data scientist and statistician, and you work in the medical domain. \\nYou have didactic skills and you can explain data analysis very well.\\nYou are about to write a Jupyter book consisting of multiple Jupyter notebooks about a given topic.\\n\\nIn front of every code-cell, add a markdown cell with an explanation of the next code cell. Write 1-3 sentences in these markdown cells.\\nWhen writing a notebook, always keep the code in the code cells concise. \\nDo only one thing and let the user see the intermediate result.\\nThen, continue with the next thing in a new code cell.\\n\\n\\nBy the end of every notebook, add a markdown cell with the headline \"## Exercise\" \\nand an exercise description allowing the reader to practically apply what they just\\nlearned in the notebook.\\n\\n\\nConfirm this with \"ok\".\\n'},\n",
       " {'role': 'assistant', 'content': 'ok'},\n",
       " {'role': 'user', 'content': 'Hi, my name is Robert Haase.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hello Robert! Nice to meet you. I\\'m ready to help you create a comprehensive Jupyter book with multiple notebooks on your chosen topic. \\n\\nAs confirmed, I\\'ll structure each notebook with:\\n- Clear markdown explanations (1-3 sentences) before each code cell\\n- Concise, focused code cells that demonstrate one concept at a time\\n- Intermediate results displayed for better understanding\\n- An \"## Exercise\" section at the end of each notebook for hands-on practice\\n\\nWhat medical data analysis topic would you like to focus on for this Jupyter book?'},\n",
       " {'role': 'user', 'content': 'What is my name?'},\n",
       " {'role': 'assistant', 'content': 'Your name is Robert Haase.'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nI would like to teach others in Foundations of Medical Data Integration and cover these aspects:\\n\\n* Files, Folders, and Filesystems\\n* Path handling and portable I/O (pathlib)\\n* Sidecar metadata (JSON/YAML), data dictionaries\\n* Local and cloud storage with fsspec (s3fs/gcsfs/adlfs), streaming I/O\\n* Next-cloud access to files\\n* Integrity checks (hashing), small-to-large file strategies\\n* Reading/writing CSV/NDJSON (pandas/polars), dtype control, missing values\\n* Schemas and validation (pandera), coercion, constraints\\n* Efficient storage: Parquet/Arrow, compression, partitioning\\n* Joins/merges, grouping, windowing; SQL-in-notebooks with DuckDB\\n* Export patterns: subsets, snapshots, reproducible filters\\n* numpy/xarray fundamentals: shapes, dtypes, chunking\\n* Array storage: HDF5/Zarr; compression (blosc/zstd), chunk size trade-offs\\n* Image I/O: tifffile/imageio; DICOM with pydicom; OME-TIFF/OME-Zarr basics\\n* CSV/Excel/NDJSON → Parquet/Arrow (batch vs streaming)\\n* In-notebook SQL analytics and joins with DuckDB/SQLite\\n* DICOM → OME-TIFF/OME-Zarr; NIfTI round-trips for neuro\\n* De-identification workflows for EHR, DICOM, and text (hashing, redaction, re-keying)\\n* NLP pipelines for clinical notes (spaCy, rule-based extraction, terminology normalization)\\n* Timezones, units, and categorical normalization\\n* Unit normalization, time alignment, resampling, and windowing for longitudinal data\\n* Entity resolution and patient matching (splink, recordlinkage) with evaluation\\n* Out-of-core/parallel conversion with Dask; progress, retries, idempotency\\n* Map to clinical vocabularies (SNOMED/LOINC/ICD/RxNorm) using Python lookup tables/services\\n\\n\\nTherefore, it would be great to have training material in the form of a Jupyter book.\\n\\nWhich Python libraries are relevant in this context? Do not write any Python code yet.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Based on your comprehensive curriculum for \"Foundations of Medical Data Integration,\" here are the key Python libraries that would be relevant:\\n\\n## Core Data Handling & I/O\\n- **pathlib** - Modern path handling\\n- **pandas/polars** - Tabular data manipulation\\n- **numpy/xarray** - Array operations and n-dimensional data\\n- **fsspec, s3fs, gcsfs, adlfs** - Unified filesystem interfaces\\n- **dask** - Parallel and out-of-core computing\\n\\n## File Formats & Storage\\n- **pyarrow** - Arrow format and Parquet I/O\\n- **h5py/tables** - HDF5 storage\\n- **zarr** - Chunked array storage\\n- **tifffile/imageio** - Image I/O\\n- **pydicom** - DICOM medical imaging\\n- **nibabel** - NIfTI neuroimaging format\\n- **ome-zarr** - OME-ZARR for microscopy\\n\\n## Data Validation & Schemas\\n- **pandera** - Data validation and schemas\\n- **pydantic** - Data validation using Python type annotations\\n- **jsonschema** - JSON schema validation\\n\\n## Database & Query\\n- **duckdb** - In-process analytical database\\n- **sqlite3** - Lightweight database\\n- **sqlalchemy** - SQL toolkit\\n\\n## Medical/Clinical Specific\\n- **spacy** - NLP processing\\n- **splink/recordlinkage** - Entity resolution and record matching\\n- **cryptography/hashlib** - Hashing and de-identification\\n\\n## Metadata & Configuration\\n- **pyyaml** - YAML parsing\\n- **json** - JSON handling\\n- **toml** - Configuration files\\n\\n## Utilities & Quality\\n- **tqdm** - Progress bars\\n- **pytest** - Testing framework\\n- **logging** - Logging and monitoring\\n- **blosc/zstd** - Compression algorithms\\n\\nThis creates a solid foundation for teaching medical data integration workflows from basic file handling through advanced clinical data processing pipelines.'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nCreate a intro.md file for a jupyter book that contains all Jupyter notebooks we just created. \\nThe introduction should give an overview in text form and with bullet points linking to the notebooks.\\nMention that the entire book is AI-generated.\\nThe repository url of the book is `https://github.com/generated-books/medical-data-integration`.\\nMention that the `generator.ipynb` file in the github repository contains all the code used for generating the book. Add a link to this file.\\nRespond the content of this file only.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"# Foundations of Medical Data Integration\\n\\nWelcome to this comprehensive guide on medical data integration using Python. This book provides practical, hands-on training for healthcare data scientists, bioinformaticians, and researchers working with diverse medical datasets.\\n\\n## About This Book\\n\\nThis entire book is AI-generated and designed to teach the essential skills needed to effectively handle, process, and integrate medical data from various sources. Each notebook combines theoretical concepts with practical implementations, ensuring you can immediately apply what you learn to real-world medical data challenges.\\n\\nThe source code and generation process for this book can be found in the GitHub repository: [https://github.com/generated-books/medical-data-integration](https://github.com/generated-books/medical-data-integration)\\n\\n**Note**: The [`generator.ipynb`](https://github.com/generated-books/medical-data-integration/blob/main/generator.ipynb) file in the repository contains all the code used for generating this book.\\n\\n## What You'll Learn\\n\\nThis book covers the complete pipeline of medical data integration, from basic file handling to advanced clinical data processing workflows. You'll master modern Python tools and techniques specifically tailored for healthcare data challenges.\\n\\n### Core Topics Covered:\\n\\n**Foundation & File Management**\\n- Files, folders, and filesystem navigation\\n- Modern path handling with pathlib\\n- Local and cloud storage integration\\n- Data integrity and validation strategies\\n\\n**Data Formats & Storage**\\n- Efficient data formats (Parquet, Arrow, HDF5, Zarr)\\n- Medical imaging formats (DICOM, OME-TIFF, NIfTI)\\n- Compression strategies and performance optimization\\n- Metadata management and data dictionaries\\n\\n**Data Processing & Integration**\\n- Advanced pandas/polars operations\\n- SQL analytics in notebooks with DuckDB\\n- Schema validation and data quality control\\n- Entity resolution and patient matching\\n\\n**Medical Data Specialization**\\n- DICOM processing and conversion workflows\\n- Clinical text processing and NLP pipelines\\n- De-identification and privacy protection\\n- Clinical vocabulary mapping (SNOMED, LOINC, ICD)\\n\\n**Scalability & Production**\\n- Out-of-core processing with Dask\\n- Streaming data workflows\\n- Reproducible data export patterns\\n- Error handling and retry strategies\\n\\n## Prerequisites\\n\\n- Basic Python programming knowledge\\n- Familiarity with data analysis concepts\\n- Understanding of healthcare data types (helpful but not required)\\n\\n## How to Use This Book\\n\\nEach notebook is designed to be self-contained while building upon previous concepts. Work through them sequentially for the best learning experience, or jump to specific topics as needed. Every notebook includes practical exercises to reinforce your learning.\\n\\nReady to master medical data integration? Let's begin!\"},\n",
       " {'role': 'user',\n",
       "  'content': '\\nBuild a table of contents in Jupytyer book yml format.\\nFirst, mention the intro.md file.\\nPlease give me the list of all notebook filenames we just created. \\nPut them in a _yml file for a Jupyter book.\\nRespond the content of this file only.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```yaml\\nformat: jb-book\\nroot: intro\\nchapters:\\n- file: 01-files-and-paths\\n  title: Files, Folders, and Path Handling\\n- file: 02-metadata-and-storage\\n  title: Metadata Management and Storage Systems\\n- file: 03-cloud-and-integrity\\n  title: Cloud Storage and Data Integrity\\n- file: 04-csv-and-validation\\n  title: CSV Processing and Data Validation\\n- file: 05-efficient-storage\\n  title: Efficient Storage with Parquet and Arrow\\n- file: 06-joins-and-sql\\n  title: Data Joins and SQL Analytics\\n- file: 07-numpy-and-arrays\\n  title: NumPy and Array Fundamentals\\n- file: 08-array-storage\\n  title: Advanced Array Storage (HDF5/Zarr)\\n- file: 09-image-and-dicom\\n  title: Medical Image I/O and DICOM Processing\\n- file: 10-batch-conversion\\n  title: Batch Data Conversion Workflows\\n- file: 11-sql-analytics\\n  title: In-Notebook SQL Analytics\\n- file: 12-medical-format-conversion\\n  title: Medical Format Conversion Pipelines\\n- file: 13-deidentification\\n  title: De-identification Workflows\\n- file: 14-clinical-nlp\\n  title: Clinical NLP and Text Processing\\n- file: 15-data-normalization\\n  title: Data Normalization and Standardization\\n- file: 16-longitudinal-data\\n  title: Longitudinal Data Processing\\n- file: 17-entity-resolution\\n  title: Entity Resolution and Patient Matching\\n- file: 18-parallel-processing\\n  title: Parallel and Out-of-Core Processing\\n- file: 19-clinical-vocabularies\\n  title: Clinical Vocabulary Mapping\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nA requirements.txt file in the `docs` folder containing all python libraries used in this Jupyter book.\\nRespond the content of this file only.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```\\n# Core data handling\\npandas>=2.0.0\\npolars>=0.20.0\\nnumpy>=1.24.0\\nxarray>=2023.1.0\\ndask>=2023.1.0\\n\\n# Path and filesystem handling\\npathlib2>=2.3.0\\nfsspec>=2023.1.0\\ns3fs>=2023.1.0\\ngcsfs>=2023.1.0\\nadlfs>=2023.1.0\\n\\n# File formats and storage\\npyarrow>=12.0.0\\nh5py>=3.8.0\\ntables>=3.8.0\\nzarr>=2.14.0\\nfastparquet>=2023.1.0\\n\\n# Medical imaging and formats\\ntifffile>=2023.1.0\\nimageio>=2.25.0\\npydicom>=2.3.0\\nnibabel>=5.0.0\\nome-zarr>=0.7.0\\n\\n# Data validation and schemas\\npandera>=0.17.0\\npydantic>=2.0.0\\njsonschema>=4.17.0\\n\\n# Database and SQL\\nduckdb>=0.9.0\\nsqlite3\\nsqlalchemy>=2.0.0\\n\\n# Clinical and NLP\\nspacy>=3.5.0\\nsplink>=3.9.0\\nrecordlinkage>=0.15.0\\n\\n# Metadata and configuration\\npyyaml>=6.0\\ntoml>=0.10.0\\n\\n# Compression\\nblosc>=1.11.0\\nzstd>=1.5.0\\n\\n# Utilities\\ntqdm>=4.65.0\\npytest>=7.2.0\\njupyter>=1.0.0\\njupyterlab>=4.0.0\\nmatplotlib>=3.6.0\\nseaborn>=0.12.0\\n\\n# Security and hashing\\ncryptography>=40.0.0\\nhashlib3>=1.0.0\\n\\n# Networking and cloud\\nrequests>=2.28.0\\nboto3>=1.26.0\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nCreate a minimal config.yml file for the jupyter book.\\nThe book will be uploaded to this github repository: https://github.com/generated-books/medical-data-integration\\nMake sure the notebooks will be executed when the book is built.\\nThe icon for the book is saved in ../icon.png\\nNote that today is September 13, 2025.\\nRespond the content of this file only.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': '```yaml\\n# Book settings\\ntitle: Foundations of Medical Data Integration\\nauthor: AI Generated Content\\nlogo: ../icon.png\\ncopyright: \"2025\"\\n\\n# Force re-execution of notebooks on each build\\nexecute:\\n  execute_notebooks: force\\n\\n# Repository settings\\nrepository:\\n  url: https://github.com/generated-books/medical-data-integration\\n  branch: main\\n\\n# HTML-specific options\\nhtml:\\n  use_edit_page_button: true\\n  use_repository_button: true\\n  use_issues_button: true\\n  home_page_in_navbar: false\\n\\n# Launch buttons\\nlaunch_buttons:\\n  notebook_interface: jupyterlab\\n  binderhub_url: https://mybinder.org\\n\\n# Information about where the book exists on the web\\ndescription: >-\\n  A comprehensive guide to medical data integration using Python.\\n  This AI-generated book covers everything from basic file handling \\n  to advanced clinical data processing workflows.\\n\\n# Parse and render settings\\nparse:\\n  myst_enable_extensions:\\n    - colon_fence\\n    - deflist\\n    - dollarmath\\n    - html_image\\n    - html_admonition\\n    - linkify\\n    - replacements\\n    - smartquotes\\n    - substitution\\n\\n# Sphinx settings\\nsphinx:\\n  config:\\n    html_show_copyright: false\\n    html_last_updated_fmt: \"%b %d, %Y\"\\n```'},\n",
       " {'role': 'user',\n",
       "  'content': '\\nWrite a Github workflow file that builds the book and uploads the content to the gh_pages branch.\\nThe book is stored in the `docs` folder of the respository.\\nRespond the content of this file only.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"```yaml\\nname: Build and Deploy Jupyter Book\\n\\non:\\n  push:\\n    branches: [ main ]\\n  pull_request:\\n    branches: [ main ]\\n\\njobs:\\n  build-and-deploy:\\n    runs-on: ubuntu-latest\\n    \\n    steps:\\n    - name: Checkout repository\\n      uses: actions/checkout@v4\\n      \\n    - name: Set up Python\\n      uses: actions/setup-python@v4\\n      with:\\n        python-version: '3.11'\\n        \\n    - name: Cache pip dependencies\\n      uses: actions/cache@v3\\n      with:\\n        path: ~/.cache/pip\\n        key: ${{ runner.os }}-pip-${{ hashFiles('docs/requirements.txt') }}\\n        restore-keys: |\\n          ${{ runner.os }}-pip-\\n          \\n    - name: Install dependencies\\n      run: |\\n        python -m pip install --upgrade pip\\n        pip install jupyter-book\\n        pip install -r docs/requirements.txt\\n        \\n    - name: Build the book\\n      run: |\\n        cd docs\\n        jupyter-book build .\\n        \\n    - name: Deploy to GitHub Pages\\n      if: github.ref == 'refs/heads/main'\\n      uses: peaceiris/actions-gh-pages@v3\\n      with:\\n        github_token: ${{ secrets.GITHUB_TOKEN }}\\n        publish_dir: docs/_build/html\\n        force_orphan: true\\n```\"},\n",
       " {'role': 'user',\n",
       "  'content': '\\nCreate a readme.md file for the jupyter book. \\nGive instructions how to build the book.\\nMention that the entire book is AI-generated. \\nMention that the `generator.ipynb` file in the github repository contains all the code used for generating the book.\\nRespond the content of this file only.\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"# Foundations of Medical Data Integration\\n\\n[![Build Status](https://github.com/generated-books/medical-data-integration/workflows/Build%20and%20Deploy%20Jupyter%20Book/badge.svg)](https://github.com/generated-books/medical-data-integration/actions)\\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\\n\\nA comprehensive, AI-generated Jupyter Book covering the foundations of medical data integration using Python. This book provides practical training for healthcare data scientists, bioinformaticians, and researchers working with diverse medical datasets.\\n\\n## 📖 Read the Book\\n\\nThe book is automatically built and deployed at: [https://generated-books.github.io/medical-data-integration](https://generated-books.github.io/medical-data-integration)\\n\\n## 🤖 AI-Generated Content\\n\\n**Important Note**: This entire book is AI-generated content. All notebooks, explanations, code examples, and exercises have been created using artificial intelligence. The [`generator.ipynb`](generator.ipynb) file in this repository contains all the code and prompts used for generating the complete book.\\n\\n## 📚 What's Covered\\n\\nThis book covers the complete pipeline of medical data integration, including:\\n\\n- **Foundation**: Files, paths, metadata, and storage systems\\n- **Data Formats**: CSV, Parquet, Arrow, HDF5, Zarr, and medical imaging formats\\n- **Medical Imaging**: DICOM, OME-TIFF, NIfTI processing and conversion\\n- **Clinical Data**: NLP, de-identification, vocabulary mapping\\n- **Advanced Topics**: Entity resolution, parallel processing, longitudinal data analysis\\n\\n## 🛠 Building the Book Locally\\n\\n### Prerequisites\\n\\n- Python 3.8 or higher\\n- Git\\n\\n### Quick Start\\n\\n1. **Clone the repository**\\n   ```bash\\n   git clone https://github.com/generated-books/medical-data-integration.git\\n   cd medical-data-integration\\n   ```\\n\\n2. **Create a virtual environment**\\n   ```bash\\n   python -m venv venv\\n   source venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\\n   ```\\n\\n3. **Install dependencies**\\n   ```bash\\n   pip install jupyter-book\\n   pip install -r docs/requirements.txt\\n   ```\\n\\n4. **Build the book**\\n   ```bash\\n   cd docs\\n   jupyter-book build .\\n   ```\\n\\n5. **View the book**\\n   ```bash\\n   # Open docs/_build/html/index.html in your browser\\n   # Or serve it locally:\\n   python -m http.server 8000 --directory _build/html\\n   ```\\n\\n### Development Mode\\n\\nFor continuous building during development:\\n\\n```bash\\ncd docs\\njupyter-book build . --builder html --all\\n```\\n\\nTo clean previous builds:\\n\\n```bash\\njupyter-book clean .\\n```\\n\\n## 📁 Repository Structure\\n\\n```\\n├── docs/                    # Jupyter Book source\\n│   ├── *.ipynb             # Individual notebook chapters\\n│   ├── intro.md            # Book introduction\\n│   ├── _config.yml         # Jupyter Book configuration\\n│   ├── _toc.yml           # Table of contents\\n│   └── requirements.txt    # Python dependencies\\n├── generator.ipynb         # AI generation code\\n├── icon.png               # Book icon\\n└── README.md              # This file\\n```\\n\\n## 🚀 Automatic Deployment\\n\\nThe book is automatically built and deployed using GitHub Actions whenever changes are pushed to the main branch. The workflow file is located at `.github/workflows/deploy.yml`.\\n\\n## 📄 License\\n\\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\\n\\n## 🤝 Contributing\\n\\nSince this is an AI-generated book, contributions should focus on:\\n- Reporting errors or issues\\n- Suggesting improvements to the generation process\\n- Adding new prompts or topics to the generator\\n\\nPlease open an issue or submit a pull request if you find any problems.\\n\\n## 📞 Support\\n\\nIf you encounter any issues while building or using this book, please:\\n1. Check the [Issues](https://github.com/generated-books/medical-data-integration/issues) page\\n2. Create a new issue with detailed information about the problem\\n3. Include your Python version, operating system, and error messages\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ea97f-aad1-474a-b289-c5b606c0c335",
   "metadata": {},
   "source": [
    "This is just an approximation of the number of tokens in the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77d34b72-bc3e-4c2f-bc67-042826128643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2553"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(chat_history).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7f540-3e34-4cd6-bdae-9fb7ab0a5b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
