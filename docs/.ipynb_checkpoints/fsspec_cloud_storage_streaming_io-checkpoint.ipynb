{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local and Cloud Storage with fsspec (s3fs/gcsfs/adlfs), Streaming I/O\n",
        "\n",
        "In medical data integration, we often need to work with data stored in various locations - from local file systems to cloud storage services like AWS S3, Google Cloud Storage, or Azure Data Lake. The `fsspec` library provides a unified interface for accessing files across different storage systems, which is crucial when dealing with large medical datasets that may be distributed across multiple platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing Required Libraries\n",
        "\n",
        "First, let's install the necessary libraries for working with different storage systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fsspec s3fs gcsfs adlfs pandas pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding fsspec\n",
        "\n",
        "Let's import the required libraries and explore the basic functionality of fsspec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fsspec\n",
        "import pandas as pd\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Local File System\n",
        "\n",
        "Let's start by creating a sample medical dataset and saving it locally to demonstrate basic file operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample medical dataset\n",
        "medical_data = pd.DataFrame({\n",
        "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
        "    'age': [45, 32, 67, 29, 55],\n",
        "    'blood_pressure_systolic': [120, 135, 145, 118, 130],\n",
        "    'blood_pressure_diastolic': [80, 85, 90, 75, 82],\n",
        "    'glucose_level': [95, 110, 125, 88, 105]\n",
        "})\n",
        "\n",
        "medical_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's save this data to a local file using fsspec's file system interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a local file system instance\n",
        "fs = fsspec.filesystem('file')\n",
        "\n",
        "# Save the data\n",
        "with fs.open('medical_data.csv', 'w') as f:\n",
        "    medical_data.to_csv(f, index=False)\n",
        "\n",
        "print(\"Data saved to medical_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's read the data back using fsspec to verify it was saved correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the data back\n",
        "with fs.open('medical_data.csv', 'r') as f:\n",
        "    df_loaded = pd.read_csv(f)\n",
        "\n",
        "df_loaded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using fsspec with URLs\n",
        "\n",
        "fsspec can automatically detect and handle different protocols. Let's demonstrate this by reading data from a public URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read a CSV file directly from a URL\n",
        "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/healthexp.csv'\n",
        "\n",
        "with fsspec.open(url, 'r') as f:\n",
        "    health_exp_data = pd.read_csv(f)\n",
        "\n",
        "health_exp_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with AWS S3\n",
        "\n",
        "Now let's explore how to work with AWS S3 storage. Note that you'll need AWS credentials configured for this to work in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an S3 file system instance\n",
        "# In practice, you would need proper AWS credentials\n",
        "s3_fs = fsspec.filesystem('s3', anon=True)  # anon=True for public buckets\n",
        "\n",
        "# Example: List files in a public S3 bucket\n",
        "try:\n",
        "    files = s3_fs.ls('s3://nyc-tlc/trip data/')\n",
        "    print(f\"Found {len(files)} files\")\n",
        "    print(\"First 3 files:\", files[:3])\n",
        "except Exception as e:\n",
        "    print(f\"Error accessing S3: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Mock Cloud Storage Example\n",
        "\n",
        "Since actual cloud storage requires credentials, let's create a mock example to demonstrate the pattern of working with cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a memory file system to simulate cloud storage\n",
        "memory_fs = fsspec.filesystem('memory')\n",
        "\n",
        "# Save medical data to \"cloud\"\n",
        "with memory_fs.open('cloud/medical/patients.json', 'w') as f:\n",
        "    medical_data.to_json(f, orient='records')\n",
        "\n",
        "print(\"Data saved to mock cloud storage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's read the data back from our mock cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read from \"cloud\"\n",
        "with memory_fs.open('cloud/medical/patients.json', 'r') as f:\n",
        "    cloud_data = pd.read_json(f)\n",
        "\n",
        "cloud_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming I/O for Large Medical Files\n",
        "\n",
        "When working with large medical imaging files or genomic data, streaming I/O becomes crucial. Let's demonstrate how to read data in chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a larger dataset to simulate streaming\n",
        "import numpy as np\n",
        "\n",
        "large_medical_data = pd.DataFrame({\n",
        "    'patient_id': [f'P{i:04d}' for i in range(10000)],\n",
        "    'measurement_1': np.random.normal(100, 15, 10000),\n",
        "    'measurement_2': np.random.normal(75, 10, 10000),\n",
        "    'measurement_3': np.random.normal(120, 20, 10000)\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "large_medical_data.to_csv('large_medical_data.csv', index=False)\n",
        "print(f\"Created dataset with {len(large_medical_data)} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's read this large file in chunks using streaming I/O to process data efficiently without loading everything into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream data in chunks\n",
        "chunk_size = 1000\n",
        "total_rows = 0\n",
        "mean_values = []\n",
        "\n",
        "with fs.open('large_medical_data.csv', 'r') as f:\n",
        "    for chunk in pd.read_csv(f, chunksize=chunk_size):\n",
        "        total_rows += len(chunk)\n",
        "        mean_values.append(chunk['measurement_1'].mean())\n",
        "        \n",
        "print(f\"Processed {total_rows} rows in {len(mean_values)} chunks\")\n",
        "print(f\"Overall mean of measurement_1: {np.mean(mean_values):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Multiple Storage Systems\n",
        "\n",
        "In medical data integration, you often need to combine data from multiple sources. Let's demonstrate how fsspec makes this seamless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data in different storage systems\n",
        "local_fs = fsspec.filesystem('file')\n",
        "memory_fs = fsspec.filesystem('memory')\n",
        "\n",
        "# Patient demographics in local storage\n",
        "demographics = pd.DataFrame({\n",
        "    'patient_id': ['P001', 'P002', 'P003'],\n",
        "    'age': [45, 32, 67],\n",
        "    'gender': ['M', 'F', 'M']\n",
        "})\n",
        "\n",
        "with local_fs.open('demographics.csv', 'w') as f:\n",
        "    demographics.to_csv(f, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lab results in \"cloud\" storage\n",
        "lab_results = pd.DataFrame({\n",
        "    'patient_id': ['P001', 'P002', 'P003'],\n",
        "    'test_date': ['2024-01-15', '2024-01-16', '2024-01-15'],\n",
        "    'hemoglobin': [14.5, 13.2, 15.1]\n",
        "})\n",
        "\n",
        "with memory_fs.open('cloud/lab_results.csv', 'w') as f:\n",
        "    lab_results.to_csv(f, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's read from both storage systems and merge the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read from local storage\n",
        "with local_fs.open('demographics.csv', 'r') as f:\n",
        "    demo_df = pd.read_csv(f)\n",
        "\n",
        "# Read from \"cloud\" storage\n",
        "with memory_fs.open('cloud/lab_results.csv', 'r') as f:\n",
        "    lab_df = pd.read_csv(f)\n",
        "\n",
        "# Merge the data\n",
        "integrated_data = pd.merge(demo_df, lab_df, on='patient_id')\n",
        "integrated_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using fsspec with Context Managers\n",
        "\n",
        "fsspec supports Python's context manager protocol, making it easy to ensure files are properly closed after use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using fsspec.open directly with any URL or path\n",
        "medical_notes = {\n",
        "    'patient_id': 'P001',\n",
        "    'notes': 'Patient presents with mild hypertension. Recommended lifestyle changes.'\n",
        "}\n",
        "\n",
        "# Write JSON data\n",
        "with fsspec.open('medical_notes.json', 'w') as f:\n",
        "    json.dump(medical_notes, f)\n",
        "\n",
        "# Read JSON data\n",
        "with fsspec.open('medical_notes.json', 'r') as f:\n",
        "    loaded_notes = json.load(f)\n",
        "\n",
        "print(loaded_notes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Caching Remote Files\n",
        "\n",
        "When working with remote medical data, caching can significantly improve performance. fsspec provides built-in caching capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a cached file system\n",
        "cached_fs = fsspec.filesystem('filecache', \n",
        "                             target_protocol='https',\n",
        "                             cache_storage='./cache')\n",
        "\n",
        "# This will cache the file locally on first access\n",
        "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/healthexp.csv'\n",
        "\n",
        "with cached_fs.open(url, 'r') as f:\n",
        "    cached_data = pd.read_csv(f)\n",
        "\n",
        "print(\"Data cached and loaded\")\n",
        "print(f\"Cache directory created: {os.path.exists('./cache')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Create a medical data integration pipeline that:\n",
        "\n",
        "1. Creates three different datasets:\n",
        "   - Patient vital signs (store locally)\n",
        "   - Laboratory results (store in memory to simulate cloud)\n",
        "   - Medication history (store as JSON locally)\n",
        "\n",
        "2. Implements a function that:\n",
        "   - Reads all three datasets using appropriate fsspec file systems\n",
        "   - Merges them based on patient_id\n",
        "   - Calculates risk scores based on the combined data\n",
        "   - Saves the final integrated dataset with risk scores\n",
        "\n",
        "3. Demonstrates streaming processing by:\n",
        "   - Creating a large dataset (>5000 records)\n",
        "   - Processing it in chunks to calculate statistics\n",
        "   - Identifying high-risk patients without loading the entire dataset into memory\n",
        "\n",
        "Your solution should showcase the flexibility of fsspec in handling different storage backends and efficient data processing techniques suitable for large medical datasets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}